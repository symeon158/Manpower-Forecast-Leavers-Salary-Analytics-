import io
from datetime import datetime

import numpy as np
import numpy as np

# --- NumPy 2.x compatibility shims for old libraries (shap, xgboost ÎºÎ»Ï€.) ---
if not hasattr(np, "float"):
    np.float = float  # deprecated alias, needed by shap
if not hasattr(np, "int"):
    np.int = int      # Î³Î¹Î± Ï„Ï…Ï‡ÏŒÎ½ ÎºÎ»Î®ÏƒÎµÎ¹Ï‚ np.int
if not hasattr(np, "bool"):
    np.bool = bool    # Î³Î¹Î± Ï„Ï…Ï‡ÏŒÎ½ ÎºÎ»Î®ÏƒÎµÎ¹Ï‚ np.bool

import pandas as pd
import streamlit as st
from prophet import Prophet
from prophet.plot import plot_plotly
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error
import optuna

# ML imports
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import (
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    ConfusionMatrixDisplay,
)
from xgboost import XGBClassifier
import shap

# CUSTOM CSS FOR KPI CARDS
# -------------------------------------------------------------------
kpi_css = """
<style>
.kpi-card {
    background: linear-gradient(135deg, #f0f4f8 0%, #dce6f2 100%);
    border-radius: 16px;
    padding: 16px 18px;
    border: 1px solid #e2e8f0;
    color: #0d1b2a;
    box-shadow: 0 3px 10px rgba(0,0,0,0.07);
    font-family: 'Segoe UI', sans-serif;
}

.kpi-label {
    font-size: 0.78rem;
    text-transform: uppercase;
    letter-spacing: 0.08em;
    color: #4a5568;
    font-weight: 700;   /* BOLD */
}

.kpi-value {
    font-size: 1.7rem;
    font-weight: 700;
    color: #1f77b4;
    margin-top: 4px;
}

.kpi-subtitle {
    font-size: 0.80rem;
    color: #4a5568;
    margin-top: 2px;
}

.kpi-badge {
    display: inline-block;
    font-size: 0.70rem;
    padding: 2px 10px;
    border-radius: 10px;
    background: #edf2f7;
    color: #1f77b4;
    margin-top: 6px;
    border: 1px solid #d1d9e0;
}
</style>
"""
st.markdown(kpi_css, unsafe_allow_html=True)




def kpi_card(label: str, value: str, subtitle: str | None = None, badge: str | None = None):
    """Render a single KPI card with custom CSS."""
    html = f"""
    <div class="kpi-card">
        <div class="kpi-label">{label}</div>
        <div class="kpi-value">{value}</div>
        {f'<div class="kpi-subtitle">{subtitle}</div>' if subtitle else ''}
        {f'<div class="kpi-badge">{badge}</div>' if badge else ''}
    </div>
    """
    st.markdown(html, unsafe_allow_html=True)

# =============================================================================
# PAGE CONFIG
# =============================================================================
st.set_page_config(
    page_title="Manpower Time Series & Leavers Salary",
    layout="wide"
)

st.title("ğŸ“ˆ Manpower Forecast & Leavers Salary Analytics (Optimized)")
st.caption(
    "Time series forecasting Î¼Îµ seasonality, salary insights, Optuna tuning, HR KPIs, "
    "ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· Prophet vs baseline ÎºÎ±Î¹ ML attrition analysis."
)

# =============================================================================
# HELPERS - DATA LOADING & PREPROCESSING
# =============================================================================
def robust_read(uploaded_file) -> pd.DataFrame:
    """
    Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ Ï„Î¿ uploaded_file Î´Î¿ÎºÎ¹Î¼Î¬Î¶Î¿Î½Ï„Î±Ï‚ Ï€Î¿Î»Î»Î­Ï‚ ÎµÎ»Î»Î·Î½Î¹ÎºÎ­Ï‚ ÎºÏ‰Î´Î¹ÎºÎ¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚.
    Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯ BytesIO, Î¿Ï€ÏŒÏ„Îµ Î´ÎµÎ½ Î¼Ï€Î»Î­ÎºÎ¿Ï…Î¼Îµ Î¼Îµ seek/position.
    Î ÎŸÎ¤Î• Î´ÎµÎ½ Ï€ÎµÏ„Î¬ÎµÎ¹ UnicodeDecodeError.
    """
    raw_bytes = uploaded_file.getvalue()  # Ï€Î¬ÏÎµ ÏŒÎ»Î± Ï„Î± bytes Î¼Î¯Î± Ï†Î¿ÏÎ¬

    encodings = ["utf-8", "cp1253", "iso-8859-7", "latin1"]

    for enc in encodings:
        try:
            return pd.read_csv(
                io.BytesIO(raw_bytes),
                sep=";",
                encoding=enc
            )
        except UnicodeDecodeError:
            continue
        except Exception:
            continue

    # Î¤ÎµÎ»Î¹ÎºÏŒ fallback â€“ Î´Î¹Î¬Î²Î±ÏƒÎµ Ï„Î± Ï€Î¬Î½Ï„Î±, Î±Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î·ÏƒÎµ Â«Ï‡Î±Î»Î±ÏƒÎ¼Î­Î½Î¿Ï…Ï‚Â» Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÎµÏ‚
    return pd.read_csv(
        io.BytesIO(raw_bytes),
        sep=";",
        encoding="latin1",
        errors="replace"
    )


@st.cache_data
def load_leavers_data(uploaded_file) -> pd.DataFrame:
    """Load and preprocess the leavers/full employee dataset."""
    df = robust_read(uploaded_file)

    # Normalize column names (Ï€Î¹Î¿ ÎµÏÏ‡ÏÎ·ÏƒÏ„Î±)
    df.columns = df.columns.str.strip().str.replace(' ', '_', regex=False).str.lower()

    # Standard names
    df = df.rename(
        columns={
            "hire_date": "HireDate",
            "departure_date": "DepartureDate",
            "job_title": "JobTitle",
            "departure_type": "Departure Type",
            "company": "Company",
            "division": "Division",
            "department": "Department",
            "ÎºÏ‰Î´Î¹ÎºÏŒÏ‚_ÎµÏÎ³Î±Î¶ÏŒÎ¼ÎµÎ½Î¿Ï…": "Registry_Number",
            "job_property": "Job Property"
        },
        errors="ignore"
    )

    # Detect salary column
    salary_col = None
    for c in df.columns:
        cl = c.lower()
        if "nominal" in cl and "sal" in cl:
            salary_col = c
            break

    if salary_col is not None:
        df.rename(columns={salary_col: "NominalSalary"}, inplace=True)

    # Parse dates
    df["HireDate"] = pd.to_datetime(df.get("HireDate"), dayfirst=True, errors="coerce")
    if "DepartureDate" in df.columns:
        df["DepartureDate"] = pd.to_datetime(df.get("DepartureDate"), dayfirst=True, errors="coerce")
    else:
        df["DepartureDate"] = pd.NaT

    # Keep rows with HireDate
    df = df.dropna(subset=["HireDate"])

    # Clean salary
    if "NominalSalary" in df.columns:
        df["NominalSalary"] = (
            df["NominalSalary"]
            .astype(str)
            .str.strip()
            .replace({"": np.nan, "-": np.nan})
            .str.replace(".", "", regex=False)   # thousands separator
            .str.replace(",", ".", regex=False)  # decimal
        )
        df["NominalSalary"] = pd.to_numeric(df["NominalSalary"], errors="coerce")
        # 2) Company list for special salary rule

        special_companies = [
            "Î‘Î›ÎŸÎ¥ÎœÎ¥Î› Î‘.Î•.",
            "CFT CARBON FIBER TECHNOLOGIES P.C.",
            "ALUTRADE Î•ÎœÎ ÎŸÎ¡Î™ÎŸ Î‘Î›ÎŸÎ¥ÎœÎ™ÎÎ™ÎŸÎ¥ Î‘.Î•.",
            "BMP Î‘.Î•.",
            "ALUSEAL A.E.",
            "GLM HELLAS Î‘Î•",
            "Î‘Î›ÎŸÎ¥ÎœÎ¥Î› Î‘Î¡Î§Î™Î¤Î•ÎšÎ¤ÎŸÎÎ™ÎšÎ‘ Î£Î¥Î£Î¤Î—ÎœÎ‘Î¤Î‘  Î‘.Î•.",
            "Î“Î‘ Î’Î™ÎŸÎœÎ—Î§. Î Î›Î‘Î£Î¤. Î¥Î›Î©Î  Î‘.Î•.",
            "BUILDING SYSTEMS INNOVATION CENTRE Î™Î”Î™Î©Î¤Î™ÎšÎ— ÎšÎ•Î¦Î‘Î›Î‘Î™ÎŸÎ¥Î§Î™ÎšÎ— Î•Î¤Î‘Î™Î¡Î•Î™Î‘",
            "ÎÎ•Î‘ Î‘Î›ÎŸÎ¥Î¦ÎŸÎÎ¤ ÎœÎŸÎÎŸÎ Î¡ÎŸÎ£Î©Î Î— Î‘ÎÎ©ÎÎ¥ÎœÎ— Î•Î¤Î‘Î™Î¡Î•Î™Î‘",
        ]

        threshold = 90  # <<< CHANGE THIS to your preferred cutoff

        if "NominalSalary" in df.columns and "Company" in df.columns:
            df.loc[
                (df["Company"].isin(special_companies)) &
                (df["NominalSalary"].notna()) &
                (df["NominalSalary"] < threshold),
                "NominalSalary"
            ] = df["NominalSalary"] * 26

    # Clean grade
    grade_cols = [col for col in df.columns if "grade" in col]
    if grade_cols:
        grade_col_name = grade_cols[0]
        df["GRADE_clean"] = (
            df[grade_col_name]
            .astype(str)
            .str.replace(",", ".", regex=False)
        )
        df["GRADE_clean"] = df["GRADE_clean"].replace({"99999": "0.1"})
        df["GRADE_clean"] = pd.to_numeric(df["GRADE_clean"], errors="coerce")
    df.drop_duplicates(subset=["Registry_Number"], inplace=True)

    return df


def build_monthly_time_series(df_slice: pd.DataFrame, selected_dep_types, start_year: int = 2019) -> pd.DataFrame:
    """
    Build a monthly time series where:
    - Hires = all HireDates
    - Departures = rows with DepartureDate, optionally filtered by Departure Type
    - Headcount = snapshot count Î±Î½Î¬ Î¼Î®Î½Î± (active employees)
    """
    if df_slice["HireDate"].isna().all():
        return pd.DataFrame(columns=["Date", "Hires", "Departures", "NetHires", "Headcount", "TurnoverRate"])

    # Monthly hires
    hires = (
        df_slice
        .groupby(df_slice["HireDate"].dt.to_period("M"))
        .size()
        .rename("Hires")
    )
    hires.index = hires.index.to_timestamp()

    # Monthly departures (filtered)
    df_departures = df_slice.dropna(subset=["DepartureDate"]).copy()
    if "Departure Type" in df_departures.columns and selected_dep_types:
        df_departures = df_departures[df_departures["Departure Type"].isin(selected_dep_types)]

    departures = (
        df_departures
        .groupby(df_departures["DepartureDate"].dt.to_period("M"))
        .size()
        .rename("Departures")
    )
    departures.index = departures.index.to_timestamp()

    # Date range
    min_hire = df_slice["HireDate"].min().replace(day=1)
    start_date = datetime(start_year, 1, 1)
    start_date = max(start_date, min_hire)

    if df_departures.empty:
        if not df_slice["HireDate"].empty:
            end_date = df_slice["HireDate"].max().replace(day=1)
        else:
            end_date = datetime.now().replace(day=1)
    else:
        end_date = df_departures["DepartureDate"].max().replace(day=1)

    monthly_index = pd.date_range(start_date, end_date, freq="MS")
    ts = pd.DataFrame(index=monthly_index)
    ts.index.name = "Date"

    ts["Hires"] = hires.reindex(monthly_index, fill_value=0)
    ts["Departures"] = departures.reindex(monthly_index, fill_value=0)
    ts["NetHires"] = ts["Hires"] - ts["Departures"]

    # Headcount per month (snapshot at first of month)
    headcounts = []
    for m in monthly_index:
        mask = (df_slice["HireDate"] <= m) & (
            df_slice["DepartureDate"].isna() | (df_slice["DepartureDate"] > m)
        )
        headcounts.append(int(mask.sum()))
    ts["Headcount"] = headcounts

    ts["TurnoverRate"] = np.where(
        ts["Headcount"] > 0,
        ts["Departures"] / ts["Headcount"] * 100,
        np.nan
    )

    ts = ts.reset_index()
    return ts


def compute_monthly_seasonality(ts: pd.DataFrame, metric: str) -> pd.DataFrame:
    """Average value per calendar month (1â€“12) for the selected metric."""
    df = ts.copy()
    df["Month"] = df["Date"].dt.month
    month_avg = (
        df.groupby("Month")[metric]
        .mean()
        .reset_index()
        .sort_values("Month")
    )
    return month_avg


def generate_departure_insights(ts: pd.DataFrame) -> str:
    """Simple auto-text insights based on departures."""
    if ts.empty or ts["Departures"].sum() == 0:
        return "Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚ ÏƒÏ„Î¿ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ Ï†Î¯Î»Ï„ÏÎ¿ ÎºÎ±Î¹ ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿."

    max_row = ts.loc[ts["Departures"].idxmax()]
    max_month = max_row["Date"].strftime("%Y-%m")
    max_value = int(max_row["Departures"])
    avg_dep = ts["Departures"].mean()

    insight = (
        f"ğŸ” *Insight Î³Î¹Î± Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚ (Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿):*\n\n"
        f"- ÎŸ Î¼Î®Î½Î±Ï‚ Î¼Îµ Ï„Î¹Ï‚ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚ ÎµÎ¯Î½Î±Î¹ Î¿ **{max_month}** "
        f"Î¼Îµ **{max_value}** Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚.\n"
        f"- ÎŸ Î¼Î­ÏƒÎ¿Ï‚ ÏŒÏÎ¿Ï‚ Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÏ‰Î½ Î±Î½Î¬ Î¼Î®Î½Î± ÎµÎ¯Î½Î±Î¹ Ï€ÎµÏÎ¯Ï€Î¿Ï… **{avg_dep:.1f}**.\n"
    )
    return insight


def get_recent_ts(ts: pd.DataFrame, months: int = 36) -> pd.DataFrame:
    """Return only the most recent N months of ts for tuning."""
    if ts.empty:
        return ts
    ts_sorted = ts.sort_values("Date")
    if ts_sorted.shape[0] <= months:
        return ts_sorted
    return ts_sorted.tail(months)

# =============================================================================
# PROPHET / OPTUNA / METRICS HELPERS
# =============================================================================
def objective(trial, ts: pd.DataFrame, metric: str):
    """Optuna objective: minimize MSE of Prophet on historical data."""
    df_prophet = ts[["Date", metric]].rename(columns={"Date": "ds", metric: "y"})

    if df_prophet["y"].sum() == 0 or df_prophet["y"].dropna().shape[0] < 3:
        return np.inf

    cp_scale = trial.suggest_float("changepoint_prior_scale", 0.001, 0.5, log=True)
    s_scale = trial.suggest_float("seasonality_prior_scale", 0.1, 30.0, log=True)
    weekly_seasonality_param = trial.suggest_categorical("weekly_seasonality", [True, False])

    m = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=weekly_seasonality_param,
        daily_seasonality=False,
        changepoint_prior_scale=cp_scale,
        seasonality_prior_scale=s_scale,
    )

    m.fit(df_prophet)

    future = df_prophet[["ds"]]
    forecast = m.predict(future)

    y_true = df_prophet["y"].values
    y_pred = forecast["yhat"].values
    mse = np.mean((y_true - y_pred) ** 2)

    return mse


@st.cache_data(show_spinner="ğŸ” Î•ÎºÏ„Î­Î»ÎµÏƒÎ· Optuna hyperparameter optimization...")
def run_optuna_tuning(ts_data: pd.DataFrame, metric: str, n_trials: int, recent_months: int = 36) -> dict:
    """Run Optuna HPO on recent subset of ts to find best Prophet params."""
    if ts_data.empty or ts_data[metric].sum() == 0 or ts_data[metric].dropna().shape[0] < 3:
        return {}

    ts_recent = get_recent_ts(ts_data, months=recent_months)
    func = lambda trial: objective(trial, ts_recent, metric)

    study = optuna.create_study(direction="minimize")
    try:
        study.optimize(func, n_trials=n_trials, show_progress_bar=False)
    except Exception:
        return {}

    return study.best_params


def calculate_metrics(df_prophet: pd.DataFrame, forecast: pd.DataFrame) -> dict:
    """Calculate MAE and MAPE on historical part of forecast."""
    df_merged = pd.merge(df_prophet, forecast, on="ds", how="inner")
    df_merged = df_merged.dropna(subset=["y", "yhat"])

    if df_merged.empty:
        return {"mae": np.nan, "mape": np.nan}

    y_true = df_merged["y"].values
    y_pred = df_merged["yhat"].values

    mae = mean_absolute_error(y_true, y_pred)

    non_zero_mask = y_true != 0
    if non_zero_mask.sum() > 0:
        mape = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100
    else:
        mape = np.nan

    return {"mae": mae, "mape": mape}


def run_prophet_forecast(
    ts: pd.DataFrame,
    metric: str,
    periods: int = 12,
    changepoint_prior_scale: float = 0.05,
    seasonality_prior_scale: float = 10.0,
    weekly_seasonality: bool = False,
):
    """Run Prophet forecast on a selected metric with tuned parameters."""
    df_prophet = ts[["Date", metric]].rename(columns={"Date": "ds", metric: "y"})

    if df_prophet["y"].sum() == 0 or df_prophet["y"].dropna().shape[0] < 3:
        return None, None, None

    m = Prophet(
        yearly_seasonality=True,
        weekly_seasonality=weekly_seasonality,
        daily_seasonality=False,
        changepoint_prior_scale=changepoint_prior_scale,
        seasonality_prior_scale=seasonality_prior_scale,
    )

    m.fit(df_prophet)

    future = m.make_future_dataframe(periods=periods, freq="MS")
    forecast = m.predict(future)

    metrics = calculate_metrics(df_prophet, forecast)

    return m, forecast, metrics


def generate_business_summary(ts: pd.DataFrame, metric: str, horizon: int, forecast: pd.DataFrame, metrics: dict) -> str:
    """Create a short HR-style narrative summary based on history and forecast."""
    if ts.empty or forecast is None or metrics is None:
        return "Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±ÏÎºÎµÏ„Î¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± Ï€Î±ÏÎ±Î³Ï‰Î³Î® Ï€ÎµÏÎ¯Î»Î·ÏˆÎ·Ï‚."

    total_hist = ts[metric].sum()
    avg_month = ts[metric].mean()
    first_date = ts["Date"].min().strftime("%Y-%m")
    last_date = ts["Date"].max().strftime("%Y-%m")

    last_history_date = ts["Date"].max()
    future_forecast = forecast[forecast["ds"] > last_history_date]
    total_future = future_forecast["yhat"].sum()

    mae = metrics.get("mae", np.nan)
    mape = metrics.get("mape", np.nan)

    summary = f"""
- Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ (ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿) ÎºÎ±Î»ÏÏ€Ï„ÎµÎ¹ Ï„Î·Î½ Ï€ÎµÏÎ¯Î¿Î´Î¿ **{first_date} â†’ {last_date}**.
- Î£Ï…Î½Î¿Î»Î¹ÎºÎ¬ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¬ {metric}: **{total_hist:.0f}**, Î¼Îµ Î¼Î­ÏƒÎ¿ ÏŒÏÎ¿ **{avg_month:.1f}** Î±Î½Î¬ Î¼Î®Î½Î±.
- Î“Î¹Î± Ï„Î¿Ï…Ï‚ ÎµÏ€ÏŒÎ¼ÎµÎ½Î¿Ï…Ï‚ **{horizon}** Î¼Î®Î½ÎµÏ‚, Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Ï€ÏÎ¿Î²Î»Î­Ï€ÎµÎ¹ ÏƒÏ…Î½Î¿Î»Î¹ÎºÎ¬ **{total_future:.0f}** {metric}.
- Î— Î¼Î­ÏƒÎ· Î±Ï€ÏŒÎ»Ï…Ï„Î· Î±Ï€ÏŒÎºÎ»Î¹ÏƒÎ· (MAE) ÎµÎ¯Î½Î±Î¹ **{mae:.2f}** ÎºÎ±Î¹ Ï„Î¿ MAPE ÎµÎ¯Î½Î±Î¹ Ï€ÎµÏÎ¯Ï€Î¿Ï… **{mape:.1f}%**, 
  Î­Î½Î´ÎµÎ¹Î¾Î· {"Î¹ÎºÎ±Î½Î¿Ï€Î¿Î¹Î·Ï„Î¹ÎºÎ®Ï‚" if not np.isnan(mape) and mape < 50 else "Î¼Î­Ï„ÏÎ¹Î±Ï‚ / Î¸Î¿ÏÏ…Î²ÏÎ´Î¿Ï…Ï‚"} Ï€ÏÎ¿ÏƒÎ±ÏÎ¼Î¿Î³Î®Ï‚, 
  ÎµÎ¹Î´Î¹ÎºÎ¬ Î±Î½ Î¿Î¹ Ï„Î¹Î¼Î­Ï‚ ÎµÎ¯Î½Î±Î¹ Î¼Î¹ÎºÏÎ¬ counts Î±Î½Î¬ Î¼Î®Î½Î±.
"""
    return summary


def compute_baseline_metrics(ts: pd.DataFrame, metric: str) -> dict | None:
    """
    Naive baseline: value of same calendar position 12 months earlier (shift(12)).
    """
    if ts.empty:
        return None

    s = ts.set_index("Date")[metric].astype(float)
    if s.dropna().shape[0] < 13:
        return None

    baseline = s.shift(12)

    mask = (~baseline.isna()) & (~s.isna())
    if mask.sum() == 0:
        return None

    y_true = s[mask].values
    y_pred = baseline[mask].values

    mae = mean_absolute_error(y_true, y_pred)
    non_zero_mask = y_true != 0
    if non_zero_mask.sum() > 0:
        mape = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100
    else:
        mape = np.nan

    return {"mae": mae, "mape": mape}


def classify_voluntary(dep_type):
    """Classify departure type as Voluntary vs Non-voluntary/Other."""
    if pd.isna(dep_type):
        return "Other / Unknown"
    s = str(dep_type).lower()

    # Î Î¡Î©Î¤Î‘ ÎµÎ»Î­Î³Ï‡Î¿Ï…Î¼Îµ Î³Î¹Î± involuntary, Î³Î¹Î± Î½Î± Î¼Î·Î½ Â«Ï€Î¹Î±ÏƒÏ„ÎµÎ¯Â» Î±Ï€ÏŒ Ï„Î¿ voluntary
    if "involuntary" in s:
        return "Non-voluntary / Other"

    # contains 'voluntary'
    if "voluntary" in s:
        return "Voluntary"

    return "Non-voluntary / Other"


# =============================================================================
# SIDEBAR â€“ FILE UPLOAD
# =============================================================================
st.sidebar.header("ğŸ“‚ Î”ÎµÎ´Î¿Î¼Î­Î½Î±")

uploaded_file = st.sidebar.file_uploader(
    "Upload leavers/full employee CSV (; separator, Greek encoding)",
    type=["csv"],
    help="Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ export Î¼Îµ Company, Division, Department, Job Title, Hire Date, Departure Date Îº.Î»Ï€."
)

if uploaded_file is None:
    st.info("ğŸ‘ˆ Î‘Î½Î­Î²Î±ÏƒÎµ Ï€ÏÏÏ„Î± Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ Ï„Ï‰Î½ ÎµÏÎ³Î±Î¶Î¿Î¼Î­Î½Ï‰Î½ (CSV).")
    st.stop()

df = load_leavers_data(uploaded_file)

# =============================================================================
# SIDEBAR â€“ FILTERS
# =============================================================================
st.sidebar.header("ğŸ” Î¦Î¯Î»Ï„ÏÎ±")

company_list = sorted(df["Company"].dropna().unique()) if "Company" in df.columns else []
selected_company = st.sidebar.selectbox("Company", ["(All)"] + list(company_list))

df_filtered = df.copy()
if selected_company != "(All)":
    df_filtered = df_filtered[df_filtered["Company"] == selected_company]

division_list = sorted(df_filtered["Division"].dropna().unique()) if "Division" in df_filtered.columns else []
selected_division = st.sidebar.selectbox("Division", ["(All)"] + list(division_list))
if selected_division != "(All)":
    df_filtered = df_filtered[df_filtered["Division"] == selected_division]

dept_list = sorted(df_filtered["Department"].dropna().unique()) if "Department" in df_filtered.columns else []
selected_department = st.sidebar.selectbox("Department", ["(All)"] + list(dept_list))
if selected_department != "(All)":
    df_filtered = df_filtered[df_filtered["Department"] == selected_department]

job_list = sorted(df_filtered["JobTitle"].dropna().unique()) if "JobTitle" in df_filtered.columns else []
selected_job = st.sidebar.selectbox("Job Title", ["(All)"] + list(job_list))
if selected_job != "(All)":
    df_filtered = df_filtered[df_filtered["JobTitle"] == selected_job]

# --- JOB PROPERTY FILTER ---
job_property_list = (
    sorted(df_filtered["Job Property"].dropna().unique())
    if "Job Property" in df_filtered.columns
    else []
)

selected_job_property = st.sidebar.selectbox(
    "Job Property",
    ["(All)"] + list(job_property_list)
)

if selected_job_property != "(All)":
    df_filtered = df_filtered[df_filtered["Job Property"] == selected_job_property]


# Departure Type filter for leavers only
df_leavers_only_for_filter = (
    df_filtered.dropna(subset=["DepartureDate"]) if "DepartureDate" in df_filtered.columns else pd.DataFrame()
)
selected_dep_types = None

if not df_leavers_only_for_filter.empty and "Departure Type" in df_leavers_only_for_filter.columns:
    dep_types = sorted(df_leavers_only_for_filter["Departure Type"].dropna().unique())
    selected_dep_types = st.sidebar.multiselect(
        "Departure Type (Filters Leavers Only)",
        options=dep_types,
        default=dep_types,
        help="Î•Ï€Î­Î»ÎµÎ¾Îµ Ï„ÏÏ€Î¿Ï…Ï‚ Î±Ï€Î¿Ï‡ÏÏÎ·ÏƒÎ·Ï‚ (Ï€.Ï‡. Voluntary, Retirement, Fixed-term)."
    )

# Early leaver threshold (months)
early_threshold = st.sidebar.slider(
    "Early leaver threshold (months)",
    min_value=3,
    max_value=24,
    value=12,
    step=1,
    help="ÎŒÏÎ¹Î¿ Î¼Î·Î½ÏÎ½ Ï…Ï€Î·ÏÎµÏƒÎ¯Î±Ï‚ ÎºÎ¬Ï„Ï‰ Î±Ï€ÏŒ Ï„Î¿ Î¿Ï€Î¿Î¯Î¿ Î¸ÎµÏ‰ÏÎ¿ÏÎ¼Îµ Î­Î½Î±Î½ leaver Ï‰Ï‚ 'early leaver'."
)

# Toggle Optuna
use_optuna = st.sidebar.checkbox("Î§ÏÎ®ÏƒÎ· Optuna tuning Î³Î¹Î± Prophet", value=False)

if df_filtered.empty:
    st.warning("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± Ï„Î± ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î± Ï†Î¯Î»Ï„ÏÎ±. Î§Î±Î»Î¬ÏÏ‰ÏƒÎµ Î»Î¯Î³Î¿ Ï„Î± Ï†Î¯Î»Ï„ÏÎ±.")
    st.stop()

st.write(
    f"- **Company:** {selected_company}  \n"
    f"- **Division:** {selected_division}  \n"
    f"- **Department:** {selected_department}  \n"
    f"- **Job Title:** {selected_job}  \n"
    f"- **Job Property:** {selected_job_property}"
)


# =============================================================================
# BUILD TIME SERIES (FULL) & DATE RANGE SLIDER
# =============================================================================
ts_full = build_monthly_time_series(df_filtered, selected_dep_types)

if ts_full.empty:
    st.warning("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÎµÏ€Î±ÏÎºÎ® Ï‡ÏÎ¿Î½Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± (Hire/Departure) Î³Î¹Î± Ï„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· ÎµÏ€Î¹Î»Î¿Î³Î®.")
    st.stop()

global_start = ts_full["Date"].min().to_pydatetime()
global_end = ts_full["Date"].max().to_pydatetime()

st.markdown("### ğŸ—“ï¸ Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ Î‘Î½Î¬Î»Ï…ÏƒÎ·Ï‚ Time Series")

period_preset = st.radio(
    "Î“ÏÎ®Î³Î¿ÏÎ· ÎµÏ€Î¹Î»Î¿Î³Î® Ï€ÎµÏÎ¹ÏŒÎ´Î¿Ï…:",
    ["ÎŒÎ»Î· Î· Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î·", "Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Î¹ 12 Î¼Î®Î½ÎµÏ‚", "Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Î¹ 24 Î¼Î®Î½ÎµÏ‚"],
    horizontal=True
)

if period_preset == "Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Î¹ 12 Î¼Î®Î½ÎµÏ‚":
    default_start = max(global_start, (global_end - pd.DateOffset(months=11)).to_pydatetime())
elif period_preset == "Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Î¹ 24 Î¼Î®Î½ÎµÏ‚":
    default_start = max(global_start, (global_end - pd.DateOffset(months=23)).to_pydatetime())
else:
    default_start = global_start

date_range = st.slider(
    "Î•Ï€Î¯Î»ÎµÎ¾Îµ Ï„Î·Î½ Ï€ÎµÏÎ¯Î¿Î´Î¿ Î±Î½Î¬Î»Ï…ÏƒÎ·Ï‚:",
    min_value=global_start,
    max_value=global_end,
    value=(default_start, global_end),
    format="YYYY-MM"
)

ts = ts_full[(ts_full["Date"] >= date_range[0]) & (ts_full["Date"] <= date_range[1])].copy()

if ts.empty:
    st.warning("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î± ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿. Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ¿ ÎµÏÏÎ¿Ï‚ Î·Î¼ÎµÏÎ¿Î¼Î·Î½Î¹ÏÎ½.")
    st.stop()

# =============================================================================
# METRIC & HORIZON
# =============================================================================
metric = st.selectbox(
    "Metric Î³Î¹Î± forecast:",
    ["Departures", "Hires", "NetHires"],
    index=0
)

horizon = st.slider(
    "Forecast horizon (Î¼Î®Î½ÎµÏ‚):",
    min_value=6,
    max_value=36,
    value=12
)

# =============================================================================
# OPTUNA TUNING PARAMS
# =============================================================================
st.sidebar.header("âš™ï¸ Prophet Tuning (Optuna HPO)")

n_trials = st.sidebar.slider(
    "Optuna Trials (Search Depth):",
    min_value=10,
    max_value=100,
    value=30,
    step=10
)

recent_months = st.sidebar.slider(
    "History used for tuning (months):",
    min_value=12,
    max_value=60,
    value=36,
    step=12,
    help="Î§ÏÎ¿Î½Î¹ÎºÏŒ Î²Î¬Î¸Î¿Ï‚ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏÎ½ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ Ï€Î¿Ï… Î¸Î± Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î·Î¸ÎµÎ¯ Î³Î¹Î± Ï„Î¿ Optuna."
)

if st.sidebar.button("Re-Run Optuna HPO"):
    st.cache_data.clear()

if use_optuna:
    best_params = run_optuna_tuning(ts, metric, n_trials, recent_months=recent_months)
else:
    best_params = {}

if best_params:
    st.sidebar.markdown("**âœ… Best Parameters Found:**")
    st.sidebar.json({k: round(v, 4) if isinstance(v, float) else v for k, v in best_params.items()})
    final_cp_scale = best_params.get("changepoint_prior_scale", 0.05)
    final_s_scale = best_params.get("seasonality_prior_scale", 10.0)
    final_weekly_seasonality = best_params.get("weekly_seasonality", False)
else:
    if use_optuna:
        st.sidebar.markdown("**âš ï¸ Optuna Î´ÎµÎ½ ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± â€“ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î¿ÏÎ½Ï„Î±Î¹ default Ï„Î¹Î¼Î­Ï‚.**")
    final_cp_scale = 0.05
    final_s_scale = 10.0
    final_weekly_seasonality = False

# =============================================================================
# RUN PROPHET FORECAST & BASELINE
# =============================================================================
model, forecast, metrics_model = run_prophet_forecast(
    ts,
    metric=metric,
    periods=horizon,
    changepoint_prior_scale=final_cp_scale,
    seasonality_prior_scale=final_s_scale,
    weekly_seasonality=final_weekly_seasonality
)

baseline_metrics = compute_baseline_metrics(ts, metric)

# =============================================================================
# KPI CARDS
# =============================================================================
# TOP KPIS (FIRST ROW)
# -------------------------------------------------------------------
col_kpi1, col_kpi2, col_kpi3, col_kpi4, col_kpi5, col_kpi6 = st.columns(6)

total_dep = int(ts["Departures"].sum())
total_hires = int(ts["Hires"].sum())
last_dep = int(ts.iloc[-1]["Departures"]) if not ts.empty else 0
period_label = f"{ts['Date'].min():%Y-%m} â†’ {ts['Date'].max():%Y-%m}"

with col_kpi1:
    kpi_card(
        label="Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚",
        value=f"{total_dep:,}",
        subtitle=period_label,
        badge="Departures"
    )

with col_kpi2:
    kpi_card(
        label="Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ Ï€ÏÎ¿ÏƒÎ»Î®ÏˆÎµÎ¹Ï‚",
        value=f"{total_hires:,}",
        subtitle=period_label,
        badge="Hires"
    )

with col_kpi3:
    kpi_card(
        label="Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿Ï‚ Î¼Î®Î½Î±Ï‚ - Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚",
        value=str(last_dep),
        subtitle=f"{ts['Date'].max():%Y-%m}" if not ts.empty else "",
        badge="Latest month"
    )

with col_kpi4:
    if metrics_model is not None and not np.isnan(metrics_model["mae"]):
        kpi_card(
            label="MAE Prophet",
            value=f"{metrics_model['mae']:.2f}",
            subtitle=f"Metric: {metric}",
            badge="Model accuracy"
        )
    else:
        kpi_card("MAE Prophet", "N/A")

with col_kpi5:
    if metrics_model is not None and not np.isnan(metrics_model["mape"]):
        kpi_card(
            label="MAPE Prophet",
            value=f"{metrics_model['mape']:.2f}%",
            subtitle=f"Metric: {metric}",
            badge="Model accuracy"
        )
    else:
        kpi_card("MAPE Prophet", "N/A")

with col_kpi6:
    if model is not None and forecast is not None:
        last_history_date = ts["Date"].max()
        future_forecast = forecast[forecast["ds"] > last_history_date]
        total_predicted_sum = int(future_forecast["yhat"].sum().round(0))
        kpi_card(
            label=f"Total Predicted {metric}",
            value=f"{total_predicted_sum:,}",
            subtitle=f"Next {horizon} months",
            badge="Forecast"
        )
    else:
        kpi_card(f"Total Predicted {metric}", "N/A")


period_label = f"{ts['Date'].min():%Y-%m} â†’ {ts['Date'].max():%Y-%m}"
st.markdown(f"**Î ÎµÏÎ¯Î¿Î´Î¿Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½ (ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ time series):** {period_label}")

st.markdown("---")

# =============================================================================
# LEAVERS DATA (FILTERED BY PERIOD & DEPARTURE TYPE)
# =============================================================================
# =============================================================================
# LEAVERS DATA (FILTERED BY PERIOD & DEPARTURE TYPE)
# =============================================================================
df_leavers_period = df_filtered.dropna(subset=["DepartureDate"]).copy()

if "Departure Type" in df_leavers_period.columns and selected_dep_types:
    df_leavers_period = df_leavers_period[df_leavers_period["Departure Type"].isin(selected_dep_types)]

# Note: df_leavers_period now contains the exact subset of employees that form the 
# Departures count in the 'ts' time series for the entire selected time range.

df_leavers_period = df_leavers_period[
    (df_leavers_period["DepartureDate"] >= date_range[0]) &
    (df_leavers_period["DepartureDate"] <= date_range[1])
]

early_kpi1, early_kpi2, early_kpi3 = st.columns(3)

if not df_leavers_period.empty:
    # ğŸ‘‰ Î’Î¬ÏƒÎ· = ÏƒÏÎ½Î¿Î»Î¿ Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÏ‰Î½ Î±Ï€ÏŒ Ï„Î¿ time series (Î¯Î´Î¹Î¿ Î¼Îµ Î£Î¥ÎÎŸÎ›Î™ÎšÎ•Î£ Î‘Î ÎŸÎ§Î©Î¡Î—Î£Î•Î™Î£)
    total_leavers_curr = total_dep  

    # Tenure
    df_leavers_period["TenureDays"] = (
        df_leavers_period["DepartureDate"] - df_leavers_period["HireDate"]
    ).dt.days
    df_leavers_period["TenureMonths"] = df_leavers_period["TenureDays"] / 30.4

    early_mask = df_leavers_period["TenureMonths"] < early_threshold
    early_count = int(early_mask.sum())
    early_pct = early_count / total_leavers_curr * 100 if total_leavers_curr > 0 else 0

    # Voluntary vs Non-voluntary (Î¼Îµ Ï‡ÏÎ®ÏƒÎ· Ï„Î·Ï‚ Î½Î­Î±Ï‚ classify_voluntary)
    df_leavers_period["VolCategory"] = df_leavers_period["Departure Type"].apply(classify_voluntary)
    vol_count = int((df_leavers_period["VolCategory"] == "Voluntary").sum())
    vol_pct = vol_count / total_leavers_curr * 100 if total_leavers_curr > 0 else 0

    with early_kpi1:
        kpi_card(
            label="Leavers (ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿Ï‚)",
            value=f"{total_leavers_curr:,}",
            subtitle=period_label,
            badge="All leavers"
        )

    with early_kpi2:
        kpi_card(
            label=f"Early leavers (<{early_threshold} Î¼Î®Î½ÎµÏ‚)",
            value=f"{early_count:,}",
            subtitle=f"{early_pct:.1f}% Ï„Î¿Ï… ÏƒÏ…Î½ÏŒÎ»Î¿Ï…",
            badge="Onboarding risk"
        )

    with early_kpi3:
        kpi_card(
            label="Voluntary departures",
            value=f"{vol_count:,}",
            subtitle=f"{vol_pct:.1f}% Ï„Î¿Ï… ÏƒÏ…Î½ÏŒÎ»Î¿Ï…",
            badge="Voluntary churn"
        )

else:
    with early_kpi1:
        kpi_card("Leavers (ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿Ï‚)", "0")
    with early_kpi2:
        kpi_card(f"Early leavers (<{early_threshold} Î¼Î®Î½ÎµÏ‚)", "0", subtitle="0.0%")
    with early_kpi3:
        kpi_card("Voluntary departures", "0", subtitle="0.0%")



st.markdown("---")

# =============================================================================
# TABS LAYOUT
# =============================================================================
tab_forecast, tab_salary, tab_churn, tab_hires, tab_ml = st.tabs([
    "ğŸ“ˆ Forecast & Time Series",
    "ğŸ’¶ Salary & Grade",
    "ğŸ¯ Churn Profiles",
    "ğŸ§² Hire Profiles",
    "ğŸ¤– Attrition ML (XGBoost & SHAP)"
])

# =============================================================================
# TAB 1: FORECAST & TIME SERIES
# =============================================================================
with tab_forecast:
    st.markdown("### ğŸ§¾ Business Summary (HR Narrative)")
    if model is not None and forecast is not None and metrics_model is not None:
        summary_text = generate_business_summary(ts, metric, horizon, forecast, metrics_model)
        st.markdown(summary_text)
    else:
        st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±ÏÎºÎµÏ„Î¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î® forecast Î³Î¹Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï€ÎµÏÎ¯Î»Î·ÏˆÎ·Ï‚.")

    st.markdown("### ğŸ“Š Î™ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ Time Series (ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿Ï‚)")
    fig_hist = px.line(
        ts,
        x="Date",
        y=metric,
        markers=True,
        title=f"Historical {metric} per Month",
    )
    fig_hist.update_layout(xaxis_title="Month", yaxis_title=metric)
    st.plotly_chart(fig_hist, use_container_width=True)

    st.markdown("### ğŸ”® Optimized Forecast & Seasonality")

    if model is None or forecast is None:
        st.warning(
            f"Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±ÏÎºÎµÏ„Î¬ Î¼Î·-Î¼Î·Î´ÎµÎ½Î¹ÎºÎ¬ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± {metric} ÏÏƒÏ„Îµ Î½Î± Î³Î¯Î½ÎµÎ¹ forecast ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿. "
            "Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ Î¬Î»Î»Î¿ metric Î® Ï€Î¹Î¿ Î³ÎµÎ½Î¹ÎºÏŒ Ï†Î¯Î»Ï„ÏÎ¿."
        )
    else:
        st.subheader("Forecast â€“ Î™ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ + ÎœÎ­Î»Î»Î¿Î½")
        fig_forecast = plot_plotly(model, forecast)
        fig_forecast.update_layout(
            xaxis_title="Month",
            yaxis_title=metric,
            legend_title="Legend",
        )
        st.plotly_chart(fig_forecast, use_container_width=True)

        st.subheader("Prophet Components (Trend & Seasonality)")
        comp_fig = model.plot_components(forecast)
        st.pyplot(comp_fig)

        st.subheader("ğŸ“† Monthly Seasonality (Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒÏ‚ Î¼Î­ÏƒÎ¿Ï‚ ÏŒÏÎ¿Ï‚ Î±Î½Î¬ Î¼Î®Î½Î±)")
        month_avg = compute_monthly_seasonality(ts, metric)
        fig_month = px.bar(
            month_avg,
            x="Month",
            y=metric,
            title=f"Average {metric} by Calendar Month",
        )
        fig_month.update_layout(xaxis_title="Month (1-12)", yaxis_title=f"Average {metric}")
        st.plotly_chart(fig_month, use_container_width=True)

    st.markdown("### âš–ï¸ Model Comparison: Prophet vs Baseline")

    if metrics_model is not None and baseline_metrics is not None:
        comp_df = pd.DataFrame([
            {"Model": "Prophet", "MAE": metrics_model["mae"], "MAPE (%)": metrics_model["mape"]},
            {"Model": "Baseline (shift 12 months)", "MAE": baseline_metrics["mae"], "MAPE (%)": baseline_metrics["mape"]},
        ])
        st.dataframe(
            comp_df.style.format({"MAE": "{:.2f}", "MAPE (%)": "{:.2f}"}),
            use_container_width=True
        )
    else:
        st.info("Î”ÎµÎ½ Î®Ï„Î±Î½ Î´Ï…Î½Î±Ï„Î® Î· ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· Prophet Î¼Îµ baseline (Î±Î½ÎµÏ€Î±ÏÎºÎ® Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± baseline Î® model).")

    if model is not None and forecast is not None:
        st.markdown("### ğŸ“‹ Forecast data table")
        display_cols = ["ds", "yhat", "yhat_lower", "yhat_upper"]
        forecast_view = forecast[display_cols].rename(
            columns={
                "ds": "Date",
                "yhat": f"{metric}_forecast",
                "yhat_lower": "Lower CI",
                "yhat_upper": "Upper CI",
            }
        )
        st.dataframe(forecast_view, use_container_width=True)

        csv_forecast = forecast_view.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            label="â¬‡ï¸ Download Forecast (CSV)",
            data=csv_forecast,
            file_name=f"{metric}_forecast_{horizon}m.csv",
            mime="text/csv",
        )

# =============================================================================
# TAB 2: SALARY & GRADE
# =============================================================================
with tab_salary:
    # Î•Ï€Î¹Î»Î¿Î³Î® Ï€Î»Î·Î¸Ï…ÏƒÎ¼Î¿Ï Î±Î½Î¬Î»Î¿Î³Î± Î¼Îµ Ï„Î¿ metric
    if metric == "Hires":
        # ÎœÎ¹ÏƒÎ¸Î¿Î¯ Î³Î¹Î± ÏŒÏƒÎ¿Ï…Ï‚ Ï€ÏÎ¿ÏƒÎ»Î®Ï†Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿
        if "HireDate" in df_filtered.columns:
            df_salary = df_filtered.copy()
            df_salary = df_salary[
                (df_salary["HireDate"] >= date_range[0]) &
                (df_salary["HireDate"] <= date_range[1])
            ]
        else:
            df_salary = pd.DataFrame()
        population_label = "Hires (ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿Ï‚)"
    else:
        # Default: Leavers ÏŒÏ€Ï‰Ï‚ Ï€ÏÎ¹Î½ (Departures Î® NetHires)
        df_salary = df_leavers_period.copy()
        population_label = "Leavers (ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿Ï‚)"

    st.markdown(f"### ğŸ’¶ Salary & Grade Analysis Î³Î¹Î± {population_label}")

    if (
        not df_salary.empty
        and "NominalSalary" in df_salary.columns
        and df_salary["NominalSalary"].notna().any()
    ):
        col1, col2 = st.columns(2)

        with col1:
            fig_sal_box = px.box(
                df_salary,
                x="JobTitle",
                y="NominalSalary",
                title=f"Distribution of Nominal Salary by Job Title ({population_label})",
            )
            fig_sal_box.update_layout(xaxis_title="Job Title", yaxis_title="Nominal Salary")
            st.plotly_chart(fig_sal_box, use_container_width=True)

        with col2:
            if "GRADE_clean" in df_salary.columns and df_salary["GRADE_clean"].notna().any():
                grade_salary = (
                    df_salary
                    .dropna(subset=["GRADE_clean"])
                    .groupby("GRADE_clean")["NominalSalary"]
                    .mean()
                    .reset_index()
                    .sort_values("GRADE_clean")
                )
                fig_grade = px.bar(
                    grade_salary,
                    x="GRADE_clean",
                    y="NominalSalary",
                    title=f"Average Nominal Salary by Grade ({population_label})",
                )
                fig_grade.update_layout(xaxis_title="Grade", yaxis_title="Average Nominal Salary")
                st.plotly_chart(fig_grade, use_container_width=True)
            else:
                st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î­Î³ÎºÏ…ÏÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ Grade Î³Î¹Î± Î½Î± Î³Î¯Î½ÎµÎ¹ Î±Î½Î¬Î»Ï…ÏƒÎ·.")
    else:
        st.info(f"Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î­Î³ÎºÏ…ÏÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ ÏƒÏ„Î¿ NominalSalary Î³Î¹Î± {population_label} ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿.")
    st.markdown("### ğŸ§© Voluntary vs Non-voluntary departures (per month)")
    if not df_leavers_period.empty:
        df_leavers_period["VolCategory"] = df_leavers_period["Departure Type"].apply(classify_voluntary)
        vol_ts = (
            df_leavers_period
            .assign(Month=lambda x: x["DepartureDate"].dt.to_period("M").dt.to_timestamp())
            .groupby(["Month", "VolCategory"])
            .size()
            .reset_index(name="Departures")
        )

        fig_vol = px.bar(
            vol_ts,
            x="Month",
            y="Departures",
            color="VolCategory",
            barmode="stack",
            title="Voluntary vs Non-voluntary departures per month"
        )
        fig_vol.update_layout(xaxis_title="Month", yaxis_title="Departures")
        st.plotly_chart(fig_vol, use_container_width=True)
    else:
        st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Î½Î± Î±Î½Î±Î»Ï…Î¸Î¿ÏÎ½ Ï‰Ï‚ voluntary/non-voluntary ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿.")

# =============================================================================
# TAB 3: CHURN PROFILES
# =============================================================================
with tab_churn:
    st.markdown("### ğŸ¯ High-Risk Profile Summary (Leavers ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿)")
    st.caption(
        "Î£ÏÎ½Î¿ÏˆÎ· Ï„Ï‰Î½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ Ï€Î¿Ï… Î­Ï‡Î¿Ï…Î½ Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹, "
        "Î¼Îµ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î¿Î½ Î¼Î­ÏƒÎ¿ Î¼Î·Î½Î¹Î±Î¯Î¿ ÎºÎ¯Î½Î´Ï…Î½Î¿ Î±Î½Î¬ ÏÏŒÎ»Î¿ ÎºÎ±Î¹ Ï„Î· ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® Î²Î±ÏÏÏ„Î·Ï„Î± (churn ratio)."
    )

    if not df_leavers_period.empty:
        total_months = ts.shape[0] if ts.shape[0] > 0 else 1

        df_profile = df_leavers_period.copy().dropna(subset=["JobTitle"])

        profile_summary = df_profile.groupby("JobTitle").agg(
            Total_Departures=("JobTitle", "size"),
            Avg_Departures_per_Month=("JobTitle", lambda x: x.size / total_months),
            Avg_Salary=("NominalSalary", "mean"),
            Most_Common_Grade=("GRADE_clean", lambda x: x.mode()[0] if not x.mode().empty else "N/A"),
            Most_Common_Departure=("Departure Type", lambda x: x.mode()[0] if not x.mode().empty else "N/A"),
        ).reset_index()

        total_leavers_grand = profile_summary["Total_Departures"].sum()
        profile_summary["Churn Ratio (%)"] = (profile_summary["Total_Departures"] / total_leavers_grand) * 100

        # ğŸ‘‰ Numeric Churn Cost Index
        profile_summary["Churn Cost Index"] = (
            profile_summary["Total_Departures"] * profile_summary["Avg_Salary"].fillna(0)
        )

        # ğŸ‘‰ ÎšÏÎ±Ï„Î¬Î¼Îµ numeric Î­ÎºÎ´Î¿ÏƒÎ· Î³Î¹Î± ÏƒÏ‰ÏƒÏ„ÏŒ sort
        profile_summary["ChurnCostIndex_num"] = profile_summary["Churn Cost Index"]

        profile_summary = profile_summary.rename(
            columns={
                "Total_Departures": "Total Leavers (History)",
                "Avg_Departures_per_Month": "Avg Departs / Month",
                "Avg_Salary": "Avg Salary (Leavers)",
                "Most_Common_Grade": "Common Grade",
                "Most_Common_Departure": "Common Departure Type",
            }
        )

        # Formatting Î³Î¹Î± ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· (Î”Î•Î Ï€ÎµÎ¹ÏÎ¬Î¶ÎµÎ¹ Ï„Î¿ numeric helper)
        profile_summary["Avg Salary (Leavers)"] = profile_summary["Avg Salary (Leavers)"].map(
            lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A"
        )
        profile_summary["Avg Departs / Month"] = profile_summary["Avg Departs / Month"].map(
            lambda x: f"{x:,.2f}" if pd.notnull(x) else "N/A"
        )
        profile_summary["Churn Ratio (%)"] = profile_summary["Churn Ratio (%)"].map(
            lambda x: f"{x:,.1f}%" if pd.notnull(x) else "N/A"
        )
        profile_summary["Churn Cost Index"] = profile_summary["Churn Cost Index"].map(
            lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A"
        )

        # Î’Î±ÏƒÎ¹ÎºÎ® Ï„Î±Î¾Î¹Î½ÏŒÎ¼Î·ÏƒÎ· Ï€Î¯Î½Î±ÎºÎ±
        profile_summary = profile_summary.sort_values("Total Leavers (History)", ascending=False)

        top_n = st.slider("Display Top N Leaver Job Titles:", min_value=5, max_value=50, value=10)

        display_cols = [
            "JobTitle",
            "Total Leavers (History)",
            "Avg Departs / Month",
            "Churn Ratio (%)",
            "Avg Salary (Leavers)",
            "Common Grade",
            "Common Departure Type",
            "Churn Cost Index",
        ]

        st.dataframe(profile_summary[display_cols].head(top_n), use_container_width=True)

       
        top3 = profile_summary.sort_values("ChurnCostIndex_num", ascending=False).head(10)
        if not top3.empty:
            st.markdown("#### ğŸ… Top 10 ÏÏŒÎ»Î¿Î¹ Î¼Îµ Ï„Î¿Î½ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ¿ 'Churn Cost Index'")
            bullets = []
            for _, row in top3.iterrows():
                bullets.append(
                    f"- **{row['JobTitle']}**: {row['Total Leavers (History)']} leavers, "
                    f"Avg Salary ~ {row['Avg Salary (Leavers)']}, "
                    f"Churn Ratio {row['Churn Ratio (%)']}, "
                    f"Churn Cost Index {row['Churn Cost Index']}"
                )
            st.markdown("\n".join(bullets))

        # Download button Î³Î¹Î± Ï€Î»Î®ÏÎµÏ‚ profile
        csv_profile = profile_summary[display_cols].to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            label="â¬‡ï¸ Download Full Churn Profile (CSV)",
            data=csv_profile,
            file_name="churn_profile_jobtitles.csv",
            mime="text/csv",
        )

        st.markdown(
            "*Note: Î¤Î¿ 'Avg Departs / Month' ÎµÎºÏ†ÏÎ¬Î¶ÎµÎ¹ Ï„Î¿Î½ Î¼Î­ÏƒÎ¿ Î¼Î·Î½Î¹Î±Î¯Î¿ Ï†ÏŒÏÏ„Î¿ Î±Î½Ï„Î¹ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·Ï‚ Î³Î¹Î± ÎºÎ¬Î¸Îµ ÏÏŒÎ»Î¿, "
            "ÎµÎ½Ï Ï„Î¿ 'Churn Cost Index' ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ¿ÏƒÎµÎ³Î³Î¹ÏƒÏ„Î¹ÎºÏŒ Î¼Î­Ï„ÏÎ¿ Ï€Î¿Ï… ÏƒÏ…Î½Î´Ï…Î¬Î¶ÎµÎ¹ ÏŒÎ³ÎºÎ¿ Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÏ‰Î½ ÎºÎ±Î¹ Î¼Î­ÏƒÎ· Î¼Î¹ÏƒÎ¸Î¿Î´Î¿ÏƒÎ¯Î±.*"
        )
    else:
        st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î±Ï€Î¿Ï‡Ï‰ÏÎ®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î·Î¸ÎµÎ¯ Ï€ÏÎ¿Ï†Î¯Î» ÎºÎ¹Î½Î´ÏÎ½Î¿Ï… ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿.")

# TAB 4: HIRE PROFILES (HIRES)
# =============================================================================
with tab_hires:
    st.markdown("### ğŸ§² High-Opportunity Profile Summary (Hires ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿)")
    st.caption(
        "Î£ÏÎ½Î¿ÏˆÎ· Ï„Ï‰Î½ Ï‡Î±ÏÎ±ÎºÏ„Î·ÏÎ¹ÏƒÏ„Î¹ÎºÏÎ½ Ï„Ï‰Î½ Ï…Ï€Î±Î»Î»Î®Î»Ï‰Î½ Ï€Î¿Ï… Ï€ÏÎ¿ÏƒÎ»Î®Ï†Î¸Î·ÎºÎ±Î½ ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿, "
        "Î¼Îµ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î¿Î½ Î¼Î­ÏƒÎ¿ Î¼Î·Î½Î¹Î±Î¯Î¿ ÏÏ…Î¸Î¼ÏŒ Ï€ÏÏŒÏƒÎ»Î·ÏˆÎ·Ï‚ Î±Î½Î¬ ÏÏŒÎ»Î¿ ÎºÎ±Î¹ Ï„Î· ÏƒÏ‡ÎµÏ„Î¹ÎºÎ® Î²Î±ÏÏÏ„Î·Ï„Î± (hire ratio)."
    )

    if "HireDate" not in df_filtered.columns:
        st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î®Î»Î· HireDate ÏƒÏ„Î¿ dataset.")
    else:
        df_hires_period = df_filtered.copy()
        df_hires_period = df_hires_period[
            (df_hires_period["HireDate"] >= date_range[0]) &
            (df_hires_period["HireDate"] <= date_range[1])
        ]

        if df_hires_period.empty:
            st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Ï€ÏÎ¿ÏƒÎ»Î®ÏˆÎµÎ¹Ï‚ Î³Î¹Î± Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î·Î¸ÎµÎ¯ Ï€ÏÎ¿Ï†Î¯Î» ÏƒÏ„Î·Î½ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î· Ï€ÎµÏÎ¯Î¿Î´Î¿.")
        elif "JobTitle" not in df_hires_period.columns:
            st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î®Î»Î· JobTitle Î³Î¹Î± Î½Î± Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î·Î¸ÎµÎ¯ hire profile.")
        else:
            total_months_h = ts.shape[0] if ts.shape[0] > 0 else 1
            df_profile_h = df_hires_period.copy().dropna(subset=["JobTitle"])

            if df_profile_h.empty:
                st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÎµÏ€Î±ÏÎºÎ® Î´ÎµÎ´Î¿Î¼Î­Î½Î± hires Î¼Îµ JobTitle.")
            else:
                # Aggregation for hires
                profile_hires = df_profile_h.groupby("JobTitle").agg(
                    Total_Hires=("JobTitle", "size"),
                    Avg_Hires_per_Month=("JobTitle", lambda x: x.size / total_months_h),
                    Avg_Salary=("NominalSalary", "mean"),
                    Most_Common_Grade=("GRADE_clean", lambda x: x.mode()[0] if not x.mode().empty else "N/A"),
                    Most_Common_JobProp=("Job Property", lambda x: x.mode()[0] if not x.mode().empty else "N/A"),
                ).reset_index()

                total_hires_grand = profile_hires["Total_Hires"].sum()
                profile_hires["Hire Ratio (%)"] = (profile_hires["Total_Hires"] / total_hires_grand) * 100

                # ğŸ‘‰ Numeric Hiring Cost Index (proxy)
                profile_hires["Hiring Cost Index"] = (
                    profile_hires["Total_Hires"] * profile_hires["Avg_Salary"].fillna(0)
                )
                profile_hires["HiringCostIndex_num"] = profile_hires["Hiring Cost Index"]

                # Rename columns for display
                profile_hires = profile_hires.rename(
                    columns={
                        "Total_Hires": "Total Hires (History)",
                        "Avg_Hires_per_Month": "Avg Hires / Month",
                        "Avg_Salary": "Avg Salary (Hires)",
                        "Most_Common_Grade": "Common Grade",
                        "Most_Common_JobProp": "Common Job Property",
                    }
                )

                # Formatting
                profile_hires["Avg Salary (Hires)"] = profile_hires["Avg Salary (Hires)"].map(
                    lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A"
                )
                profile_hires["Avg Hires / Month"] = profile_hires["Avg Hires / Month"].map(
                    lambda x: f"{x:,.2f}" if pd.notnull(x) else "N/A"
                )
                profile_hires["Hire Ratio (%)"] = profile_hires["Hire Ratio (%)"].map(
                    lambda x: f"{x:,.1f}%" if pd.notnull(x) else "N/A"
                )
                profile_hires["Hiring Cost Index"] = profile_hires["Hiring Cost Index"].map(
                    lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A"
                )

                # Sort by total hires
                profile_hires = profile_hires.sort_values("Total Hires (History)", ascending=False)

                top_n_h = st.slider(
                    "Display Top N Hire Job Titles:",
                    min_value=5,
                    max_value=50,
                    value=10
                )

                display_cols_h = [
                    "JobTitle",
                    "Total Hires (History)",
                    "Avg Hires / Month",
                    "Hire Ratio (%)",
                    "Avg Salary (Hires)",
                    "Common Grade",
                    "Common Job Property",
                    "Hiring Cost Index",
                ]

                st.dataframe(profile_hires[display_cols_h].head(top_n_h), use_container_width=True)

                top10_h = profile_hires.sort_values("HiringCostIndex_num", ascending=False).head(10)
                if not top10_h.empty:
                    st.markdown("#### ğŸ… Top 10 ÏÏŒÎ»Î¿Î¹ Î¼Îµ Ï„Î¿Î½ Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ¿ 'Hiring Cost Index'")
                    bullets_h = []
                    for _, row in top10_h.iterrows():
                        bullets_h.append(
                            f"- **{row['JobTitle']}**: {row['Total Hires (History)']} hires, "
                            f"Avg Salary ~ {row['Avg Salary (Hires)']}, "
                            f"Hire Ratio {row['Hire Ratio (%)']}, "
                            f"Hiring Cost Index {row['Hiring Cost Index']}"
                        )
                    st.markdown("\n".join(bullets_h))

                # Download button Î³Î¹Î± Ï€Î»Î®ÏÎµÏ‚ hire profile
                csv_profile_h = profile_hires[display_cols_h].to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    label="â¬‡ï¸ Download Full Hire Profile (CSV)",
                    data=csv_profile_h,
                    file_name="hire_profile_jobtitles.csv",
                    mime="text/csv",
                )

                st.markdown(
                    "*Note: Î¤Î¿ 'Avg Hires / Month' ÎµÎºÏ†ÏÎ¬Î¶ÎµÎ¹ Ï„Î¿Î½ Î¼Î­ÏƒÎ¿ Î¼Î·Î½Î¹Î±Î¯Î¿ ÏÏ…Î¸Î¼ÏŒ Ï€ÏÎ¿ÏƒÎ»Î®ÏˆÎµÏ‰Î½ Î±Î½Î¬ ÏÏŒÎ»Î¿, "
                    "ÎµÎ½Ï Ï„Î¿ 'Hiring Cost Index' ÎµÎ¯Î½Î±Î¹ Ï€ÏÎ¿ÏƒÎµÎ³Î³Î¹ÏƒÏ„Î¹ÎºÏŒ Î¼Î­Ï„ÏÎ¿ Ï€Î¿Ï… ÏƒÏ…Î½Î´Ï…Î¬Î¶ÎµÎ¹ ÏŒÎ³ÎºÎ¿ Ï€ÏÎ¿ÏƒÎ»Î®ÏˆÎµÏ‰Î½ ÎºÎ±Î¹ Î¼Î­ÏƒÎ· Î¼Î¹ÏƒÎ¸Î¿Î´Î¿ÏƒÎ¯Î±.*"
                )

# =============================================================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import shap
import streamlit as st

from datetime import datetime
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import (
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    ConfusionMatrixDisplay,
)
from xgboost import XGBClassifier


# ---------------------------------------------------------------------
# ğŸ”§ Helper: safe SHAP plotting for Streamlit + Matplotlib
# ---------------------------------------------------------------------
def shap_plot_safely(plot_func):
    """
    Wrapper to avoid SHAP/Matplotlib axis issues in Streamlit.
    - Clears any previous figures
    - Runs the SHAP plot function (which uses global plt.gcf())
    - Grabs the current figure and sends it to Streamlit
    - Clears/close again
    """
    plt.clf()
    plt.close("all")

    plot_func()  # this should call a shap.*plot(..., show=False)

    fig = plt.gcf()
    st.pyplot(fig)

    plt.clf()
    plt.close("all")


# ---------------------------------------------------------------------
# ğŸ§  SHAP â€“ Advanced Explainability
# ---------------------------------------------------------------------
def render_shap_advanced(
    best_xgb_ml,
    X_train_ml: pd.DataFrame,
    X_active_ml: pd.DataFrame,
    likely_to_attrite_ml: pd.DataFrame,
    registry_numbers_ml: pd.Series,
    meta_cols: pd.DataFrame,
):
    """
    Advanced SHAP analysis & visualizations for the ML attrition model.

    Parameters
    ----------
    best_xgb_ml : trained XGBClassifier
    X_train_ml : pd.DataFrame
        Training feature matrix.
    X_active_ml : pd.DataFrame
        Features for active employees (aligned with predictions).
    likely_to_attrite_ml : pd.DataFrame
        Subset of active_df_full_ml Î¼Îµ Predicted_Attrition == 1.
    registry_numbers_ml : pd.Series
        Registry numbers aligned on original index.
    meta_cols : pd.DataFrame
        Î ÎµÏÎ¹Î­Ï‡ÎµÎ¹ First Name, Last Name, Division, Department, Job Position.
    """

    st.markdown("## ğŸ§  SHAP â€“ Model Explainability")

    if best_xgb_ml is None or X_train_ml is None or X_train_ml.empty:
        st.info("Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ ML Î´ÎµÎ½ ÎµÎ¯Î½Î±Î¹ Î´Î¹Î±Î¸Î­ÏƒÎ¹Î¼Î¿ Î® Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± SHAP.")
        return

    # ------------------------------------------------------------------
    # 0ï¸âƒ£ PER-EMPLOYEE TOP DRIVERS TABLE (PREDICTED LEAVERS)
    # ------------------------------------------------------------------
    st.markdown("### ğŸ“Œ Predicted Attrition & Top SHAP Drivers (Active Employees)")

    num_likely_ml = 0 if likely_to_attrite_ml is None else likely_to_attrite_ml.shape[0]
    st.write(f"Employees predicted to attrite (next period): **{num_likely_ml}**")

    if num_likely_ml > 0:
        with st.spinner("Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ Ï„Î¿Ï€Î¹ÎºÏÎ½ SHAP drivers Î³Î¹Î± predicted leavers..."):
            X_likely = X_active_ml.loc[likely_to_attrite_ml.index].copy()
            X_likely = X_likely.apply(pd.to_numeric, errors="coerce").fillna(0.0)

            explainer_local = shap.TreeExplainer(best_xgb_ml)
            shap_likely_all = explainer_local.shap_values(X_likely, check_additivity=False)

            # Deal with binary/multiclass: keep class 1 (Attrition=1) if list
            if isinstance(shap_likely_all, list):
                class_idx = 1 if len(shap_likely_all) > 1 else 0
                shap_likely = shap_likely_all[class_idx]
            else:
                shap_likely = shap_likely_all

            feature_names = X_likely.columns
            top_drivers_list = []

            for i in range(X_likely.shape[0]):
                row_vals = shap_likely[i]
                idx_sorted = np.argsort(-np.abs(row_vals))
                top_idx_row = idx_sorted[:3]

                drivers = []
                for j in top_idx_row:
                    fname = feature_names[j]
                    contrib = row_vals[j]
                    sign = "â†‘" if contrib > 0 else "â†“"
                    drivers.append(f"{fname} ({sign})")

                top_drivers_list.append(", ".join(drivers))

            output_df = pd.DataFrame(
                {
                    "Registry Number": registry_numbers_ml.loc[likely_to_attrite_ml.index],
                    "ÎŒÎ½Î¿Î¼Î±": meta_cols["First Name"].loc[likely_to_attrite_ml.index],
                    "Î•Ï€ÏÎ½Ï…Î¼Î¿": meta_cols["Last Name"].loc[likely_to_attrite_ml.index],
                    "Division": meta_cols["Division"].loc[likely_to_attrite_ml.index],
                    "Department": meta_cols["Department"].loc[likely_to_attrite_ml.index],
                    "Job Title": meta_cols["Job Position"].loc[likely_to_attrite_ml.index],
                    "Attrition Probability": likely_to_attrite_ml["Attrition_Probability"],
                    "Top 3 SHAP Drivers": top_drivers_list,
                },
                index=likely_to_attrite_ml.index,
            ).sort_values("Attrition Probability", ascending=False)

            st.dataframe(output_df.head(50), use_container_width=True)

            csv_pred = output_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label="â¬‡ï¸ Download Predicted Attrition Employees (CSV)",
                data=csv_pred,
                file_name="predicted_attrition_employees_with_drivers.csv",
                mime="text/csv",
            )
    else:
        st.info(
            "Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÎµÎ½ÎµÏÎ³Î¿Î¯ ÎµÏÎ³Î±Î¶ÏŒÎ¼ÎµÎ½Î¿Î¹ Ï€Î¿Ï… Î½Î± Ï€ÏÎ¿Î²Î»Î­Ï€Î¿Î½Ï„Î±Î¹ Ï‰Ï‚ leavers Î¼Îµ Ï„Î¿ Ï„ÏÎ­Ï‡Î¿Î½ threshold."
        )

    # Ensure numeric train matrix
    X_train_num = X_train_ml.apply(pd.to_numeric, errors="coerce").fillna(0.0)

    # ------------------------------------------------------------------
    # 1ï¸âƒ£ GLOBAL SHAP SUMMARIES (DOT + BAR)
    # ------------------------------------------------------------------
    with st.expander("ğŸ“Š Global SHAP Feature Importance (Summary)", expanded=True):
        with st.spinner("Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ SHAP values (sample)..."):
            # Sample for speed
            sample_size_ml = min(1000, X_train_num.shape[0])
            idx_sample_ml = np.random.choice(
                X_train_num.shape[0], size=sample_size_ml, replace=False
            )
            X_sample = X_train_num.iloc[idx_sample_ml].copy()

            explainer = shap.TreeExplainer(best_xgb_ml)
            shap_values_all = explainer.shap_values(X_sample, check_additivity=False)

            # Handle binary/multiclass: choose class 1 (Attrition=1) if list
            if isinstance(shap_values_all, list):
                class_idx = 1 if len(shap_values_all) > 1 else 0
                shap_values = shap_values_all[class_idx]
            else:
                shap_values = shap_values_all

            # SHAP summary dot plot
            st.markdown("#### ğŸ”µ SHAP Summary Plot (Dot)")
            shap_plot_safely(
                lambda: shap.summary_plot(
                    shap_values,
                    X_sample,
                    feature_names=X_sample.columns,
                    plot_type="dot",
                    show=False,
                    color_bar=False,
                )
            )

            # SHAP summary bar plot
            st.markdown("#### ğŸŸ¦ SHAP Summary Plot (Bar)")
            shap_plot_safely(
                lambda: shap.summary_plot(
                    shap_values,
                    X_sample,
                    feature_names=X_sample.columns,
                    plot_type="bar",
                    show=False,
                    color_bar=False,
                )
            )

    # ------------------------------------------------------------------
    # 2ï¸âƒ£ DEPENDENCE PLOTS (Top feature + user-selected)
    # ------------------------------------------------------------------
    with st.expander("ğŸ” SHAP Dependence Plots (Top Drivers)", expanded=False):
        # Î‘Î½ Î³Î¹Î± ÎºÎ¬Ï€Î¿Î¹Î¿ Î»ÏŒÎ³Î¿ Î´ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ shap_values Î±Ï€ÏŒ Ï€Î¬Î½Ï‰, Ï„Î± Ï…Ï€Î¿Î»Î¿Î³Î¯Î¶Î¿Ï…Î¼Îµ Î¾Î±Î½Î¬
        if "shap_values" not in locals():
            sample_size_ml = min(800, X_train_num.shape[0])
            idx_sample_ml = np.random.choice(
                X_train_num.shape[0], size=sample_size_ml, replace=False
            )
            X_sample = X_train_num.iloc[idx_sample_ml].copy()

            explainer = shap.TreeExplainer(best_xgb_ml)
            shap_values_all = explainer.shap_values(X_sample, check_additivity=False)
            if isinstance(shap_values_all, list):
                class_idx = 1 if len(shap_values_all) > 1 else 0
                shap_values = shap_values_all[class_idx]
            else:
                shap_values = shap_values_all

        # Mean absolute impact per feature
        mean_abs = np.abs(shap_values).mean(axis=0)
        top_idx = int(np.argmax(mean_abs))
        top_feature = X_sample.columns[top_idx]

        # 2Î¿ Ï€Î¹Î¿ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ feature Î³Î¹Î± interaction coloring
        second_idx = int(np.argsort(-mean_abs)[1]) if X_sample.shape[1] > 1 else top_idx
        second_feature = X_sample.columns[second_idx]

        st.markdown(
            f"#### 1ï¸âƒ£ Dependence plot Î³Î¹Î± ÎºÏÏÎ¹Î¿ driver: **{top_feature}** "
            f"(color = {second_feature})"
        )

        shap_plot_safely(
            lambda: shap.dependence_plot(
                top_feature,
                shap_values,
                X_sample,
                interaction_index=second_feature,
                show=False,
            )
        )

        # Î•Ï€Î¹Î»Î¿Î³Î® Î¬Î»Î»Î¿Ï… feature Î±Ï€ÏŒ Ï„Î¿Î½ Ï‡ÏÎ®ÏƒÏ„Î·
        feature_choice = st.selectbox(
            "Î•Ï€Î¯Î»ÎµÎ¾Îµ Î¬Î»Î»Î¿ feature Î³Î¹Î± dependence plot:",
            options=list(X_sample.columns),
            index=top_idx,
        )
        st.markdown(f"#### 2ï¸âƒ£ Dependence plot Î³Î¹Î± **{feature_choice}**")

        shap_plot_safely(
            lambda: shap.dependence_plot(
                feature_choice,
                shap_values,
                X_sample,
                show=False,
            )
        )

    # ------------------------------------------------------------------
    # 3ï¸âƒ£ OPTIONAL: INTERACTION SUMMARY (cached)
    # ------------------------------------------------------------------
    with st.expander("ğŸ”— SHAP Feature Interactions (optional)", expanded=False):

        # ÎœÎ¹ÎºÏÏŒ toggle Î³Î¹Î± Î½Î± Î¼Î·Î½ Ï„ÏÎ­Ï‡ÎµÎ¹ ÎºÎ±Î½ Î±Î½ Î´ÎµÎ½ Ï„Î¿ Î¸ÎµÏ‚
        enable_interactions = st.checkbox(
            "Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ / ÎµÎ¼Ï†Î¬Î½Î¹ÏƒÎ· interactions (Î²Î±ÏÎ¹Î¬ Ï€ÏÎ¬Î¾Î· â€“ Î¼ÏŒÎ½Î¿ ÏŒÏ„Î±Î½ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹)",
            value=False,
            key="show_shap_interactions",
        )

        if not enable_interactions:
            st.info("Î•Î½ÎµÏÎ³Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï„Î¿ checkbox Î³Î¹Î± Î½Î± Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î¿ÏÎ½ Ï„Î± SHAP interaction values.")
        else:
            try:
                # Î‘Î½ Î´ÎµÎ½ Ï„Î± Î­Ï‡Î¿Ï…Î¼Îµ Î®Î´Î· ÏƒÏ„Î· session_state â†’ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ ÎœÎ™Î‘ Ï†Î¿ÏÎ¬
                if "shap_interactions" not in st.session_state:
                    with st.spinner("Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ SHAP interaction values (Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î±ÏÎ³Î®ÏƒÎµÎ¹ Î»Î¯Î³Î¿)..."):
                        expl_inter = shap.TreeExplainer(best_xgb_ml)
                        shap_inter_all = expl_inter.shap_interaction_values(X_sample)

                        # Î‘Î½ ÎµÎ¯Î½Î±Î¹ list (multiclass), Ï€Î±Î¯ÏÎ½Î¿Ï…Î¼Îµ Ï„Î·Î½ Attrition=1 ÎºÎ»Î¬ÏƒÎ·
                        if isinstance(shap_inter_all, list):
                            class_idx = 1 if len(shap_inter_all) > 1 else 0
                            shap_inter_to_plot = shap_inter_all[class_idx]
                        else:
                            shap_inter_to_plot = shap_inter_all

                        # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· ÏƒÏ„Î· session_state Î³Î¹Î± reuse
                        st.session_state["shap_interactions"] = shap_inter_to_plot
                        st.session_state["shap_interactions_X_sample"] = X_sample
                        st.session_state["shap_interactions_model_id"] = id(best_xgb_ml)

                else:
                    # Î‘Î½ Î­Ï‡Î¿Ï…Î¼Îµ Î±Ï€Î¿Î¸Î·ÎºÎµÏ…Î¼Î­Î½Î± interactions, Î±Î»Î»Î¬Î¶ÎµÎ¹ Î¼ÏŒÎ½Î¿ Î±Î½ Î¬Î»Î»Î±Î¾Îµ Î¼Î¿Î½Ï„Î­Î»Î¿
                    if st.session_state.get("shap_interactions_model_id") != id(best_xgb_ml):
                        st.session_state.pop("shap_interactions", None)
                        st.session_state.pop("shap_interactions_X_sample", None)
                        st.session_state.pop("shap_interactions_model_id", None)
                        st.warning(
                            "Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î¬Î»Î»Î±Î¾Îµ â€“ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ ÎµÎº Î½Î­Î¿Ï… Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ interactions. "
                            "ÎÎ±Î½Î±Ï„ÏƒÎ­ÎºÎ±ÏÎµ Ï„Î¿ checkbox."
                        )
                        st.stop()

                    shap_inter_to_plot = st.session_state["shap_interactions"]
                    X_sample = st.session_state["shap_interactions_X_sample"]

                st.markdown("#### ğŸ”— SHAP Interaction Summary (Top pairwise effects)")

                shap_plot_safely(
                    lambda: shap.summary_plot(
                        shap_inter_to_plot,
                        X_sample,
                        show=False,
                    )
                )

            except TypeError:
                st.info("Î— Ï„Ï‰ÏÎ¹Î½Î® Î­ÎºÎ´Î¿ÏƒÎ· Ï„Î¿Ï… SHAP Î´Ï…ÏƒÎºÎ¿Î»ÎµÏÎµÏ„Î±Î¹ Î¼Îµ interaction values Î³Î¹Î± Î±Ï…Ï„ÏŒ Ï„Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿.")
            except Exception as e:
                st.info(f"Î”ÎµÎ½ Î®Ï„Î±Î½ Î´Ï…Î½Î±Ï„ÏŒÏ‚ Î¿ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ interaction values: {e}")

    # ------------------------------------------------------------------
    # 4ï¸âƒ£ TOP DRIVERS Î“Î™Î‘ Î£Î¥Î“ÎšÎ•ÎšÎ¡Î™ÎœÎ•ÎÎŸ Î•Î¡Î“Î‘Î–ÎŸÎœÎ•ÎÎŸ
    # ------------------------------------------------------------------
    with st.expander("ğŸŒŠ SHAP â€“ Top drivers Î³Î¹Î± ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ ÎµÏÎ³Î±Î¶ÏŒÎ¼ÎµÎ½Î¿", expanded=False):
        if likely_to_attrite_ml is None or likely_to_attrite_ml.empty:
            st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ predicted leavers Î³Î¹Î± Î±Î½Î¬Î»Ï…ÏƒÎ·.")
        else:
            # Registry numbers Ï„Ï‰Î½ predicted leavers
            reg_series = registry_numbers_ml.loc[likely_to_attrite_ml.index]
            reg_list = reg_series.astype(str).tolist()

            selected_reg = st.selectbox(
                "Î•Ï€Î¯Î»ÎµÎ¾Îµ Registry Number:",
                options=reg_list,
            )

            # index Ï„Î¿Ï… ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿Ï…
            idx_selected = reg_series[reg_series.astype(str) == selected_reg].index[0]

            # ğŸ”¹ Full Name
            first_name = meta_cols.loc[idx_selected, "First Name"]
            last_name = meta_cols.loc[idx_selected, "Last Name"]
            full_name = f"{first_name} {last_name}"

            # 1 Î³ÏÎ±Î¼Î¼Î® features Î³Î¹Î± Ï„Î¿Î½ ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ ÎµÏÎ³Î±Î¶ÏŒÎ¼ÎµÎ½Î¿
            row_X = X_active_ml.loc[[idx_selected]].copy()
            row_X = row_X.apply(pd.to_numeric, errors="coerce").fillna(0.0)
            row_X.columns = row_X.columns.astype(str)

            expl_local = shap.TreeExplainer(best_xgb_ml)
            shap_values_all = expl_local.shap_values(row_X, check_additivity=False)

            # --- Î•Ï€Î¹Î»Î¿Î³Î® ÏƒÏ‰ÏƒÏ„Î®Ï‚ ÎºÎ»Î¬ÏƒÎ·Ï‚ & 1D vector ---
            if isinstance(shap_values_all, list):
                class_idx = 1 if len(shap_values_all) > 1 else 0
                shap_row = shap_values_all[class_idx][0]
                base_val_all = expl_local.expected_value
                if isinstance(base_val_all, (list, np.ndarray)):
                    base_val = base_val_all[class_idx]
                else:
                    base_val = base_val_all
            else:
                shap_row = shap_values_all[0]
                base_val = expl_local.expected_value
                if isinstance(base_val, (list, np.ndarray)):
                    base_val = base_val[0]

            # Î¦Ï„Î¹Î¬Ï‡Î½Î¿Ï…Î¼Îµ Ï€Î¯Î½Î±ÎºÎ± Î¼Îµ SHAP values
            feature_names = row_X.columns.tolist()
            df_local = pd.DataFrame({
                "Feature": feature_names,
                "SHAP": shap_row,
                "AbsSHAP": np.abs(shap_row),
            }).sort_values("AbsSHAP", ascending=False)

            top_n = st.slider("Î Î»Î®Î¸Î¿Ï‚ ÎºÎ¿ÏÏ…Ï†Î±Î¯Ï‰Î½ drivers", 3, 20, 10)
            df_top = df_local.head(top_n)

            st.markdown(
                f"#### ğŸŒŠ Top {top_n} SHAP drivers Î³Î¹Î± **{full_name}** (Registry: {selected_reg})"
            )

            # Horizontal bar chart (not SHAP's own plotting, so normal Matplotlib is fine)
            fig, ax = plt.subplots(figsize=(8, 6))
            ax.barh(df_top["Feature"], df_top["SHAP"])
            ax.invert_yaxis()
            ax.set_xlabel("SHAP value (ÏƒÏ…Î¼Î²Î¿Î»Î® ÏƒÏ„Î·Î½ Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„Î± Î±Ï€Î¿Ï‡ÏÏÎ·ÏƒÎ·Ï‚)")
            ax.set_ylabel("Feature")
            st.pyplot(fig)
            plt.clf()
            plt.close("all")


# ---------------------------------------------------------------------
# ğŸ¤– MAIN: Attrition ML (XGBoost & SHAP)
# ---------------------------------------------------------------------
def run_ml_prediction(uploaded_file, df_filtered):
    st.markdown("### ğŸ¤– Attrition Prediction (Optimized XGBoost + SHAP)")
    st.caption(
        "Î¤Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Ï‰Ï‚ Ï€ÏÎ¿Ï‚ **recall** (Î½Î± Ï€Î¹Î¬ÏƒÎµÎ¹ ÏŒÏƒÎ¿ Ï„Î¿ Î´Ï…Î½Î±Ï„ÏŒÎ½ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿Ï…Ï‚ Î¼ÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÎ¿ÏÏ‚ leavers). "
        "Î— Î±Ï€ÏŒÏ†Î±ÏƒÎ· Î³Î¹Î± Attrition=1 Î³Î¯Î½ÎµÏ„Î±Î¹ Î¼Îµ ÏÏ…Î¸Î¼Î¹Î¶ÏŒÎ¼ÎµÎ½Î¿ threshold."
    )

    # Î‘Î½ Î±Î»Î»Î¬Î¾ÎµÎ¹ Î±ÏÏ‡ÎµÎ¯Î¿, Î±Î½Î±Î³ÎºÎ±ÏƒÏ„Î¹ÎºÎ¬ retrain
    if "current_file_name" not in st.session_state or st.session_state["current_file_name"] != uploaded_file.name:
        st.session_state["current_file_name"] = uploaded_file.name
        st.session_state.pop("xgb_model", None)
        st.session_state["xgb_needs_retrain"] = True

    # --- 1. Reload raw file for ML ---
    uploaded_file.seek(0)
    df_ml = robust_read(uploaded_file)
    df_ml.columns = df_ml.columns.str.strip()

    # Rename columns (ÏƒÏÎ¼Ï†Ï‰Î½Î± Î¼Îµ Ï„Î¿ Excel screenshot)
    rename_map = {
        "ÎšÏ‰Î´Î¹ÎºÏŒÏ‚ ÎµÏÎ³Î±Î¶ÏŒÎ¼ÎµÎ½Î¿Ï…": "Registry Number",
        "ÎŒÎ½Î¿Î¼Î±": "First Name",
        "ÎŸÎ½Î¿Î¼Î±": "First Name",
        "Î•Ï€ÏÎ½Ï…Î¼Î¿": "Last Name",
        "Î¦ÏÎ»Î¿": "Gender",
        "Î—Î»Î¹ÎºÎ¯Î±": "Age",
        "Î—Î¼/Î½Î¯Î± Î³Î­Î½Î½Î·ÏƒÎ·Ï‚": "Birth Date",
        "Î—Î¼/Î½Î¯Î± Î³Î­Î½Î½": "Birth Date",
        "hire_date": "Hire Date",
        "departure_date": "Departure Date",
        "departure_type": "Departure Reason Description",
        "NominalSalary": "Nominal Salary",
        "Î£Ï‡Î­ÏƒÎ· Î•ÏÎ³Î±ÏƒÎ¯Î±Ï‚": "Work Relationship",
        "Î£Ï‡Î­ÏƒÎ· Î•ÏÎ³": "Work Relationship",
        "Î ÎµÏÎ¹Î³ÏÎ±Ï†Î® Î¥Ï€Î¿ÎºÎ±Ï„Î±ÏƒÏ„Î®Î¼Î±Ï„Î¿Ï‚": "City",
        "Division": "Division",
        "Job Property": "Job Property",
        "Î™Î´Î¹ÏŒÏ„Î·Ï„Î± Î Ï": "Job Property",
        "job_title": "Job Position",
        "GRADE": "Grade",
        "Department": "Department",
        "ÎŸÎ¹ÎºÎ¿Î³ÎµÎ½ÎµÎ¹Î±ÎºÎ® ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·": "Marital Status",
        "NominalSa": "Nominal Salary",
        "Î¦Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ® ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±": "Tax Category",
        "Î’Î±Î¸Î¼Î¯Î´Î± Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·Ï‚": "Education Level",
        "Î’Î±Î¸Î¼Î¯Î´Î± Î•Îº": "Education Level",
        "Company": "Company",
    }
    df_ml.rename(columns=rename_map, inplace=True)

    today = datetime.today()

    # ÎšÏÎ±Ï„Î¬Î¼Îµ Î¼ÎµÏ„Î±-Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î³Î¹Î± output predicted leavers
    meta_cols_dict = {}
    meta_base_cols = ["Registry Number", "First Name", "Last Name", "Division", "Department", "Job Position"]
    for col in meta_base_cols:
        if col in df_ml.columns:
            meta_cols_dict[col] = df_ml[col].copy()
        else:
            meta_cols_dict[col] = pd.Series(index=df_ml.index, dtype="object")
    meta_cols = pd.DataFrame(meta_cols_dict)

    # --- 3. Date features & Attrition ---
    date_columns_ml = ["Birth Date", "Hire Date", "Departure Date"]
    for col in date_columns_ml:
        if col in df_ml.columns:
            df_ml[col] = pd.to_datetime(
                df_ml[col],
                format="%d/%m/%Y",
                errors="coerce",
                dayfirst=True,
            )

    # Calculate Tenure (Î¸Î± drop-Î±ÏÎ¹ÏƒÏ„ÎµÎ¯ Î±ÏÎ³ÏŒÏ„ÎµÏÎ± Î³Î¹Î± leakage)
    if "Hire Date" in df_ml.columns and "Departure Date" in df_ml.columns:
        df_ml["Tenure"] = (
            df_ml["Departure Date"].fillna(today) - df_ml["Hire Date"]
        ).dt.days // 365
    else:
        st.error(
            "ÎŸÎ¹ ÏƒÏ„Î®Î»ÎµÏ‚ 'Hire Date' Î® 'Departure Date' Î´ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î¼ÎµÏ„Î¬ Ï„Î· Î¼ÎµÏ„Î¿Î½Î¿Î¼Î±ÏƒÎ¯Î±. Î‘Î´ÏÎ½Î±Ï„Î· Î· Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·."
        )
        st.stop()

    # Calculate Attrition target
    if "Departure Date" in df_ml.columns:
        df_ml["Attrition"] = df_ml["Departure Date"].notnull().astype(int)
    else:
        st.error(
            "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ„Î®Î»Î· 'Departure Date' ÏƒÏ„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ â€“ Î´ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Î¿ÏÎ¹ÏƒÏ„ÎµÎ¯ Attrition."
        )
        st.stop()

    # --- 4. Business filters & cleaning ---
    if "Work Relationship" not in df_ml.columns:
        st.error(
            "Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ„Î®Î»Î· 'Î£Ï‡Î­ÏƒÎ· Î•ÏÎ³Î±ÏƒÎ¯Î±Ï‚' (Work Relationship). Î”ÎµÎ½ Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± ÎµÏ†Î±ÏÎ¼Î¿ÏƒÏ„ÎµÎ¯ ML attrition Î¼Î¿Î½Ï„Î­Î»Î¿."
        )
        st.stop()

    if "Department" in df_ml.columns:
        df_ml.loc[
            df_ml["Department"].astype(str).str.contains("Î•Î Î‘ÎÎ‘Î¤Î™ÎœÎŸÎ›ÎŸÎ“Î—Î£Î—", na=False),
            "Departure Reason Description",
        ] = "ÎœÎ•Î¤Î‘Î¦ÎŸÎ¡Î‘ Î£Î• Î‘Î›Î›Î— Î•Î¤Î‘Î™Î¡Î•Î™Î‘"

    # Î•Ï„Î±Î¹ÏÎµÎ¯Î± / ÏƒÏ‡Î­ÏƒÎ· ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ / voluntary only
    if "Company" in df_ml.columns:
        df_ml = df_ml[df_ml["Company"] == "Î‘Î›ÎŸÎ¥ÎœÎ¥Î› Î‘.Î•."]

    df_ml = df_ml[df_ml["Work Relationship"] == "Î‘ÎŸÎ¡Î™Î£Î¤ÎŸÎ¥ Î§Î¡ÎŸÎÎŸÎ¥"]
    df_ml = df_ml[
        (df_ml["Departure Reason Description"] == "VOLUNTARY DEPARTURE")
        | (df_ml["Departure Reason Description"].isnull())
    ]

    if "Departure Date" in df_ml.columns:
        df_ml = df_ml[
            (df_ml["Departure Date"] > "2018-12-31")
            | (df_ml["Departure Date"].isnull())
        ]

    # --- 5. Salary Cleaning and Log Transform ---
    if "Nominal Salary" in df_ml.columns:
        df_ml["Nominal Salary"] = (
            df_ml["Nominal Salary"]
            .astype(str)
            .str.replace("[", "", regex=False)
            .str.replace("]", "", regex=False)
            .str.replace(",", ".", regex=False)
            .str.strip()
        )
        df_ml["Nominal Salary"] = pd.to_numeric(
            df_ml["Nominal Salary"], errors="coerce"
        )

        # Clean extreme low values (e.g. day/weekly)
        df_ml["Nominal Salary"] = np.where(
            df_ml["Nominal Salary"] < 90,
            df_ml["Nominal Salary"] * 26,  # Assuming low values are weekly/daily
            df_ml["Nominal Salary"],
        )
        df_ml["Nominal Salary"].fillna(
            df_ml["Nominal Salary"].median(), inplace=True
        )

        # Log transform Î³Î¹Î± Î½Î± Î¼ÎµÎ¹ÏÏƒÎ¿Ï…Î¼Îµ skewness
        df_ml["Log Nominal Salary"] = np.log1p(df_ml["Nominal Salary"])
    else:
        st.error("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ„Î®Î»Î· Nominal Salary.")
        st.stop()

    # --- 6. Grade Cleaning ---
    if "Grade" in df_ml.columns:
        df_ml["Grade"] = (
            df_ml["Grade"]
            .astype(str)
            .str.replace("[", "", regex=False)
            .str.replace("]", "", regex=False)
            .str.replace(",", ".", regex=False)
            .str.strip()
        )
        df_ml["Grade"] = df_ml["Grade"].replace({"99999": "0.1"})
        df_ml["Grade"] = pd.to_numeric(df_ml["Grade"], errors="coerce")

    # --- 7. Basic checks for Registry Number ---
    if "Registry Number" not in df_ml.columns:
        st.error("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÏƒÏ„Î®Î»Î· 'Î‘ÏÎ¹Î¸Î¼ÏŒÏ‚ Î¼Î·Ï„ÏÏÎ¿Ï…' (Registry Number).")
        st.stop()

    # Drop rows Ï‡Ï‰ÏÎ¯Ï‚ Registry Number & drop duplicates on Registry Number
    df_ml = df_ml.dropna(subset=["Registry Number"])
    df_ml.drop_duplicates(subset="Registry Number", inplace=True)

    # --- 8. Convert numeric fields ---
    if "Gender" in df_ml.columns:
        df_ml["Gender"] = pd.to_numeric(df_ml["Gender"], errors="coerce")

    if "Job Property" in df_ml.columns:
        df_ml["Job Property"] = df_ml["Job Property"].map(
            {"ADMINISTRATIVE": 1, "OPERATIONAL": 0}
        )

    if "Tax Category" in df_ml.columns:
        df_ml["Tax Category"] = (
            df_ml["Tax Category"]
            .astype(str)
            .str.replace("[", "", regex=False)
            .str.replace("]", "", regex=False)
            .str.strip()
        )
        df_ml["Tax Category"] = pd.to_numeric(
            df_ml["Tax Category"], errors="coerce"
        )

    if "Age" in df_ml.columns:
        df_ml["Age"] = pd.to_numeric(df_ml["Age"], errors="coerce")

    # --- 9. Drop leaking / unused columns + Tenure ---
    registry_numbers_ml = df_ml["Registry Number"].copy()

    df_ml.drop(
        columns=[
            "Departure Date",
            "Hire Date",
            "Work Relationship",
            "Registry Number",
            "Departure Reason Description",
            "Birth Date",
            "Company",
            "Nominal Salary",  # Dropping original salary after log transform
            "Education Level",
            "First Name",
            "Last Name",
        ],
        inplace=True,
        errors="ignore",
    )

    # --- 10. Drop NaN ÏƒÎµ Î²Î±ÏƒÎ¹ÎºÎ¬ numeric features Ï€ÏÎ¹Î½ Ï„Î± dummies ---
    core_numeric = [
        col
        for col in ["Gender", "Age", "Tax Category", "Log Nominal Salary", "Grade"]
        if col in df_ml.columns
    ]
    if core_numeric:
        df_ml = df_ml.dropna(subset=core_numeric)

    # --- 11. Dummies & feature matrix ---
    categorical_columns_ml = [
        col
        for col in ["City", "Division", "Job Position", "Department", "Marital Status"]
        if col in df_ml.columns
    ]

    df_transformed_ml = pd.get_dummies(
        df_ml, columns=categorical_columns_ml, drop_first=True
    )
    df_transformed_ml.columns = df_transformed_ml.columns.astype(str)

    if "Attrition" not in df_transformed_ml.columns:
        st.error(
            "Î”ÎµÎ½ Î¼Ï€ÏŒÏÎµÏƒÎµ Î½Î± ÎµÎ½Ï„Î¿Ï€Î¹ÏƒÏ„ÎµÎ¯ Î· ÏƒÏ„Î®Î»Î· ÏƒÏ„ÏŒÏ‡Î¿Ï… Attrition Î¼ÎµÏ„Î¬ Ï„Î± dummies."
        )
        st.stop()

    # Drop any remaining NaNs after transformations
    df_transformed_ml.dropna(inplace=True)

    X_ml = df_transformed_ml.drop(columns=["Attrition"])
    y_ml = df_transformed_ml["Attrition"]

    # Convert bool â†’ int
    bool_cols = X_ml.select_dtypes(include=["bool"]).columns
    X_ml[bool_cols] = X_ml[bool_cols].astype(int)

    # Ensure numeric
    X_ml = X_ml.apply(pd.to_numeric, errors="coerce").fillna(0.0)

    st.write(
        f"ğŸ“ Î Î±ÏÎ±Ï„Î·ÏÎ®ÏƒÎµÎ¹Ï‚ Î³Î¹Î± ML: {X_ml.shape[0]} rows, {X_ml.shape[1]} features. "
        f"Attrition rate: {y_ml.mean():.2%}"
    )

    # --- 13. Train/Test split ---
    X_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(
        X_ml, y_ml, test_size=0.2, stratify=y_ml, random_state=42
    )

    # --- Imbalance Weight ---
    neg_ml = np.sum(y_train_ml == 0)
    pos_ml = np.sum(y_train_ml == 1)
    scale_pos_weight_ml = neg_ml / max(pos_ml, 1)

    st.markdown("### âš™ï¸ Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· XGBoost (Î²ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿ ÏƒÎµ Recall)")
    if "xgb_needs_retrain" not in st.session_state:
        st.session_state["xgb_needs_retrain"] = True

    train_button = st.button("ğŸ” Train / update attrition model")

    if train_button:
        st.session_state["xgb_needs_retrain"] = True

    if "xgb_model" not in st.session_state or st.session_state["xgb_needs_retrain"]:
        param_grid_ml = {
            "n_estimators": [200, 300, 500],
            "max_depth": [3, 5, 7],
            "learning_rate": [0.01, 0.05, 0.1, 0.2],
            "gamma": [0, 0.1, 0.5],
            "subsample": [0.7, 0.8, 1.0],
            "colsample_bytree": [0.7, 0.8, 1.0],
            "reg_alpha": [0.001, 0.01, 0.1, 1],
            "reg_lambda": [0.001, 0.01, 0.1, 1],
        }

        xgb_clf = XGBClassifier(
            scale_pos_weight=scale_pos_weight_ml,
            random_state=42,
            use_label_encoder=False,
            eval_metric="logloss",
            n_estimators=500,
        )

        kf_ml = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        n_iter_search = 50

        with st.spinner(
            f"ğŸ” Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· XGBoost + RandomizedSearchCV ({n_iter_search} iterations, 5-fold, scoring=recall)..."
        ):
            random_search_ml = RandomizedSearchCV(
                estimator=xgb_clf,
                param_distributions=param_grid_ml,
                n_iter=n_iter_search,
                cv=kf_ml,
                scoring="recall",
                verbose=0,
                n_jobs=-1,
                refit=True,
                random_state=42,
            )

            random_search_ml.fit(X_train_ml, y_train_ml)
            best_xgb_ml = random_search_ml.best_estimator_
            best_xgb_ml.fit(X_train_ml, y_train_ml)

        st.session_state["xgb_model"] = best_xgb_ml
        st.session_state["xgb_best_params"] = random_search_ml.best_params_
        st.session_state["xgb_train_columns"] = X_train_ml.columns.tolist()
        st.session_state["xgb_needs_retrain"] = False

        st.success("âœ… ÎÎ­Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ XGBoost ÎµÎºÏ€Î±Î¹Î´ÎµÏÏ„Î·ÎºÎµ.")

        # ğŸ”„ Invalidate cached SHAP interactions after retrain
        for key in ["shap_interactions", "shap_interactions_X_sample", "shap_interactions_model_id"]:
            st.session_state.pop(key, None)

    else:
        best_xgb_ml = st.session_state["xgb_model"]
        X_train_ml = X_train_ml[st.session_state["xgb_train_columns"]]
        st.info("Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ Ï„Î¿ Î®Î´Î· ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Î¼Î­Î½Î¿ XGBoost Î¼Î¿Î½Ï„Î­Î»Î¿ (Î´ÎµÎ½ Î­Î³Î¹Î½Îµ retrain).")

    st.write("Best XGBoost parameters (last training):")
    st.json(st.session_state.get("xgb_best_params", {}))

    # --- 15. Evaluation on test set ---
    st.markdown("### ğŸ“Š Test Set Performance (Î¼Îµ Î­Î¼Ï†Î±ÏƒÎ· ÏƒÏ„Î· Recall)")
    threshold = st.slider(
        "Decision threshold Î³Î¹Î± Attrition = 1",
        min_value=0.1,
        max_value=0.9,
        value=0.4,
        step=0.05,
        help="ÎœÎ¹ÎºÏÏŒÏ„ÎµÏÎ¿ threshold â†’ Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ¿Ï…Ï‚ Ï€ÏÎ¿Î²Î»ÎµÏ€ÏŒÎ¼ÎµÎ½Î¿Ï…Ï‚ leavers (Ï…ÏˆÎ·Î»ÏŒÏ„ÎµÏÎ· recall, Ï‡Î±Î¼Î·Î»ÏŒÏ„ÎµÏÎ· precision).",
    )

    # Ï†ÏÎ¿Î½Ï„Î¯Î¶Î¿Ï…Î¼Îµ Î¿Î¹ ÏƒÏ„Î®Î»ÎµÏ‚ Ï„Î¿Ï… test Î½Î± ÎµÎ¯Î½Î±Î¹ Î¯Î´Î¹ÎµÏ‚ Î¼Îµ Ï„Î¿Ï… train
    X_test_ml = X_test_ml[st.session_state["xgb_train_columns"]]

    y_test_proba_ml = best_xgb_ml.predict_proba(X_test_ml)[:, 1]
    y_test_pred_ml = (y_test_proba_ml >= threshold).astype(int)

    auc_ml = roc_auc_score(y_test_ml, y_test_proba_ml)
    precision_ml = precision_score(y_test_ml, y_test_pred_ml)
    recall_val = recall_score(y_test_ml, y_test_pred_ml)
    f1_ml = f1_score(y_test_ml, y_test_pred_ml)

    st.write(f"ROC AUC: **{auc_ml:.3f}**")
    st.write(f"Recall (target metric): **{recall_val:.3f}**")
    st.write(f"Precision: **{precision_ml:.3f}**")
    st.write(f"F1 Score: **{f1_ml:.3f}**")

    st.text("Classification Report (Test Set):")
    st.text(classification_report(y_test_ml, y_test_pred_ml, digits=3))

    cm_ml = confusion_matrix(y_test_ml, y_test_pred_ml)
    fig_cm, ax_cm = plt.subplots()
    disp_ml = ConfusionMatrixDisplay(
        cm_ml, display_labels=["No Attrition", "Attrition"]
    )
    disp_ml.plot(cmap="Blues", ax=ax_cm)
    st.pyplot(fig_cm)
    plt.clf()
    plt.close("all")

    # --- 16. Predict attrition for active employees ---
    active_indices_ml = df_transformed_ml[
        df_transformed_ml["Attrition"] == 0
    ].index
    active_df_full_ml = df_transformed_ml.loc[active_indices_ml].copy()
    active_df_full_ml["Registry Number"] = registry_numbers_ml.loc[active_indices_ml]

    X_active_ml = active_df_full_ml.drop(
        columns=["Attrition", "Registry Number"], errors="ignore"
    )

    # Ensure same columns as training data
    missing_cols_ml = set(st.session_state["xgb_train_columns"]) - set(X_active_ml.columns)
    for col in missing_cols_ml:
        X_active_ml[col] = 0
    X_active_ml = X_active_ml[st.session_state["xgb_train_columns"]]

    active_df_full_ml["Attrition_Probability"] = best_xgb_ml.predict_proba(
        X_active_ml
    )[:, 1]
    active_df_full_ml["Predicted_Attrition"] = (
        active_df_full_ml["Attrition_Probability"] >= threshold
    ).astype(int)

    likely_to_attrite_ml = active_df_full_ml[
        active_df_full_ml["Predicted_Attrition"] == 1
    ]

    # ğŸ”™ Î•Ï€Î¹ÏƒÏ„ÏÎ­Ï†Î¿Ï…Î¼Îµ ÏŒ,Ï„Î¹ Ï‡ÏÎµÎ¹Î¬Î¶ÎµÏ„Î±Î¹ Ï„Î¿ SHAP
    return {
        "best_xgb_ml": best_xgb_ml,
        "X_train_ml": X_train_ml,
        "X_active_ml": X_active_ml,
        "likely_to_attrite_ml": likely_to_attrite_ml,
        "registry_numbers_ml": registry_numbers_ml,
        "meta_cols": meta_cols,
    }


# ---------------------------------------------------------------------
# ğŸ“Œ Î§ÏÎ®ÏƒÎ· Î¼Î­ÏƒÎ± ÏƒÏ„Î¿ tab
# ---------------------------------------------------------------------
with tab_ml:
    results = run_ml_prediction(uploaded_file, df_filtered)

    # Î‘Î½ Ï„Î¿ training / prediction Ï€Î­Ï„Ï…Ï‡Îµ, ÎºÎ¬Î½Î¿Ï…Î¼Îµ SHAP analysis
    if results is not None:
        render_shap_advanced(**results)
